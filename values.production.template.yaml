osm-seed:

  # ====================================================================================================
  # ====================================================================================================
  # ==================================Global Configurations=============================================
  # ====================================================================================================
  # ====================================================================================================
  environment: production
  # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
  cloudProvider: aws

  # ====================================================================================================
  # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
  # ====================================================================================================
  AWS_S3_BUCKET: {{PRODUCTION_S3_BUCKET}}

  # AWS SSL ARN
  AWS_SSL_ARN: {{AWS_SSL_ARN}}

  # Specify serviceType.
  #
  # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
  # Use `NodePort` for local testing on minikube.
  #
  # The recommended setting is `ClusterIP`, and then following the instructions to
  # point a DNS record to the cluster IP address. This will setup the ingress rules
  # for all services as subdomains and configure SSL using Lets Encrypt.
  #
  # If you specify `LoadBalancer` as the service type, if you also specify
  # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
  # as the SSL certificate for your services. Else, you will need to configure
  # SSL separately. 
  serviceType: ClusterIP
  createClusterIssuer: true
  # Domain that is pointed to the clusterIP
  # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
  # Then, the cluster configuration will setup services at their respective subdomains:
  # - web.osmseed.example.com
  # - overpass.osmseed.example.com
  # - nominatim.osmseed.example.com
  # - etc.
  domain: openhistoricalmap.org

  # ====================================================================================================
  # Configuration for Lets Encrypt setup
  # ====================================================================================================

  # Admin Email address used when generating Lets Encrypt certificates.
  # You will be notified of expirations, etc. on this email address.
  adminEmail: ohm-admins@googlegroups.com

  # ====================================================================================================
  # Variables for osm-seed database
  # ====================================================================================================
  db:
    enabled: true
    priorityClass: 'high-priority'
    # For node selector you should create the node with a label "nodegroup_type"
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: api_db_xlarge
    env:
      POSTGRES_DB: {{PRODUCTION_DB}}
      POSTGRES_USER: {{PRODUCTION_DB_USER}}
      POSTGRES_PASSWORD: {{PRODUCTION_DB_PASSWORD}}
      LOG_STATEMENT: "none"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-db
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID : vol-08b85b3e370f2d690
      AWS_ElasticBlockStore_size: 600Gi
    resources:
      enabled: true
      requests:
        memory: "13Gi"
        cpu: "2800m"
      limits:
        memory: "14Gi"
        cpu: "3800m"
    sharedMemorySize: 2Gi
    postgresqlConfig:
      enabled: true
      values: |
        listen_addresses = '*'
        max_connections = 200
        shared_buffers = 4GB
        work_mem = 20MB
        maintenance_work_mem = 512MB
        dynamic_shared_memory_type = posix
        effective_io_concurrency = 200
        max_wal_size = 1GB
        min_wal_size = 256MB
        random_page_cost = 1.0
        effective_cache_size = 8GB
        log_min_duration_statement = 3000
        log_connections = on
        log_disconnections = on
        log_duration = off
        log_lock_waits = on
        log_statement = 'none'
        log_timezone = 'Etc/UTC'
        datestyle = 'iso, mdy'
        timezone = 'Etc/UTC'
        lc_messages = 'en_US.utf8'
        lc_monetary = 'en_US.utf8'
        lc_numeric = 'en_US.utf8'
        lc_time = 'en_US.utf8'
        default_text_search_config = 'pg_catalog.english'
        # Parallelism settings
        max_parallel_workers_per_gather = 2
        max_parallel_workers = 4
        max_worker_processes = 4
        parallel_tuple_cost = 0.05
        parallel_setup_cost = 500
        min_parallel_table_scan_size = 2MB
        min_parallel_index_scan_size = 256kB
        session_preload_libraries = 'auto_explain'
        auto_explain.log_min_duration = '3s'
  # ====================================================================================================
  # Variables for osm-seed website
  # ====================================================================================================
  web:
    enabled: true
    priorityClass: 'high-priority'
    replicaCount: 1
    # Set staticIp, if you are using cloudProvider=gcp
    staticIp: c
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: www.openhistoricalmap.org
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_large
    env:
      MAILER_ADDRESS: {{MAILER_ADDRESS}}
      MAILER_DOMAIN: openhistoricalmap.org
      MAILER_USERNAME: {{MAILER_USERNAME}}
      MAILER_PASSWORD: {{MAILER_PASSWORD}}
      OSM_id_key: {{PRODUCTION_ID_APPLICATION}}
      OAUTH_CLIENT_ID: {{PRODUCTION_OAUTH_CLIENT_ID}}
      OAUTH_KEY: {{PRODUCTION_OAUTH_KEY}}
      MAILER_FROM: ohm-admins@googlegroups.com
      NOMINATIM_URL: nominatim-api.openhistoricalmap.org
      OVERPASS_URL: overpass-api.openhistoricalmap.org
      NEW_RELIC_LICENSE_KEY: 'none'
      NEW_RELIC_APP_NAME: 'none'
      ORGANIZATION_NAME: OpenHistoricalMap
      WEBSITE_STATUS: 'online'
      RAILS_CREDENTIALS_YML_ENC: {{PRODUCTION_RAILS_CREDENTIALS_YML_ENC}}
      RAILS_MASTER_KEY: {{PRODUCTION_RAILS_MASTER_KEY}}
      RAILS_ENV: production
      RAILS_LOG_LEVEL: info
      RAILS_STORAGE_SERVICE: s3
      RAILS_STORAGE_REGION: us-east-1
      RAILS_STORAGE_BUCKET: ohm-website-production
    resources:
      enabled: false
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "3Gi"
        cpu: "1500m"
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 4
      cpuUtilization: 50
    sharedMemorySize: 512Mi
    livenessProbeExec: true
  # ====================================================================================================
  # Variables for memcached. Memcached is used to store session cookies
  # ====================================================================================================
  memcached:
    enabled: true
    priorityClass: 'high-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_large
    resources:
      enabled: false
      requests:
        memory: "8Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "2"

  # ====================================================================================================
  # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
  # ====================================================================================================
  planetDump:
    enabled: true
    priorityClass: 'medium-priority'
    schedule: '0 0 * * *'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: job
    env:
      OVERWRITE_PLANET_FILE: false
    resources:
      enabled: false
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for full-history container
  # ====================================================================================================
  fullHistory:
    enabled: true
    priorityClass: 'low-priority'
    schedule: '0 0 * * 0'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: job
    env:
      OVERWRITE_FHISTORY_FILE: false
    resources:
      enabled: false
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"
        
  # ====================================================================================================
  # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  replicationJob:
    enabled: true
    priorityClass: 'high-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_large
    env:
      ENABLE_SEND_SLACK_MESSAGE: "true"
      SLACK_WEBHOOK_URL: {{OHM_SLACK_WEBHOOK_URL}}
    resources:
      enabled: false
      requests:
        memory: "20Gi"
        cpu: "8"
      limits:
        memory: "24Gi"
        cpu: "10"

  # ====================================================================================================
  # Variables for osm-seed to pupulate the apidb
  # ====================================================================================================
  populateApidb:
    enabled: false
    priorityClass: 'low-priority'
    env:
      URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
    resources:
      enabled: false
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "2.5"

  # ====================================================================================================
  # Variables to start a pod to process osm files
  # ====================================================================================================
  osmProcessor:
    enabled: false
    priorityClass: 'low-priority'
    env: 
      URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
      OSM_FILE_ACTION: simple_pbf
    resources:
      enabled: false
      requests:
        memory: "14Gi"
        cpu: "4"
      limits:
        memory: "16Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for restoring the DB
  # ====================================================================================================

  dbBackupRestore:
    cronjobs:
    - name: web-db
      enabled: true
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: true
        BACKUP_CLOUD_FOLDER: database/web-api-db
        BACKUP_CLOUD_FILE: ohm-api-web-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
    - name: tm-db
      enabled: true
      schedule: '0 1 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: true
        BACKUP_CLOUD_FOLDER: database/tm-db
        BACKUP_CLOUD_FILE: ohm-tm-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
      resources:
        enabled: false
        requests:
          memory: '300Mi'
          cpu: '0.5'
        limits:
          memory: '400Mi'
          cpu: '0.6'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
    - name: osmcha-db
      enabled: false
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: 'true'
        BACKUP_CLOUD_FOLDER: database/osmcha-db
        BACKUP_CLOUD_FILE: osmseed-osmcha-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for tiler-db
  # ====================================================================================================

  tilerDb:
    enabled: false
    priorityClass: 'medium-priority'
    useExternalHost: # When we are using useExternalHost.enabled= true other variables are giong to be disable ans use the external host config
      enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    env:
      POSTGRES_HOST: {{PRODUCTION_TILER_DB_HOST}}
      POSTGRES_DB: tiler_osm_production
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: {{PRODUCTION_TILER_DB_PASSWORD}}
      POSTGRES_PORT: 5432
    sharedMemorySize: 2Gi
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-d
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID: vol-0e4d738f35f5361fe
      AWS_ElasticBlockStore_size: 200Gi
    resources:
      enabled: true
      requests:
        memory: "29Gi"
        cpu: "7500m"
      limits:
        memory: "29Gi"
        cpu: "7600m"
    postgresqlConfig:
      enabled: true
      values: |
        listen_addresses = '*'
        max_connections = 100
        shared_buffers = 10GB
        work_mem = 256MB
        maintenance_work_mem = 2GB
        dynamic_shared_memory_type = posix
        effective_io_concurrency = 300
        max_wal_size = 4GB
        min_wal_size = 512MB
        random_page_cost = 1.0
        effective_cache_size = 24GB
        log_min_duration_statement = 15000
        log_connections = on
        log_disconnections = on
        log_duration = off
        log_lock_waits = on
        log_statement = 'none'
        log_timezone = 'Etc/UTC'
        datestyle = 'iso, mdy'
        timezone = 'Etc/UTC'
        lc_messages = 'en_US.utf8'
        lc_monetary = 'en_US.utf8'
        lc_numeric = 'en_US.utf8'
        lc_time = 'en_US.utf8'
        default_text_search_config = 'pg_catalog.english'

        # Parallelism settings
        max_parallel_workers_per_gather = 8
        max_parallel_workers = 16
        max_worker_processes = 16
        parallel_tuple_cost = 0.05
        parallel_setup_cost = 500
        min_parallel_table_scan_size = 8MB
        min_parallel_index_scan_size = 512kB

        # Enable auto_explain and pg_stat_statements
        shared_preload_libraries = 'auto_explain'
        auto_explain.log_min_duration = '10s'

        # Timeout settings
        tcp_keepalives_idle = 300
        tcp_keepalives_interval = 60
        tcp_keepalives_count = 10

        # Disable join options for routes
        enable_mergejoin = true
        enable_hashjoin = true

        # Timeout settings for queries
        statement_timeout = '600s'
        lock_timeout = '60s'
        idle_in_transaction_session_timeout = '300s'

        # pg_stat_statements settings
        pg_stat_statements.max = 15000
        pg_stat_statements.track = none
        
  # ====================================================================================================
  # Variables for tiler-imposm
  # ====================================================================================================

  tilerImposm:
    enabled: false
    priorityClass: 'medium-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    env:
      TILER_IMPORT_FROM: osm
      TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-250117_0002.osm.pbf
      REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
      SEQUENCE_NUMBER: "1690000"
      OVERWRITE_STATE: false
      UPLOAD_EXPIRED_FILES: true
      IMPORT_NATURAL_EARTH: true
      IMPORT_OSM_LAND: true
      IMPOSM3_IMPORT_LAYERS: "all"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID: vol-02dfd0aac5821f25b
      AWS_ElasticBlockStore_size: 50Gi
    resources:
      enabled: true
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "2Gi"
        cpu: "1"
  # ====================================================================================================
  # Variables for tiler-server
  # ====================================================================================================

  tilerServer:
    enabled: true
    priorityClass: 'medium-priority'
    externalService:
      enabled: true
      ip: {{PRODUCTION_TILER_SERVER_HOST}}
      port: 9090
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    replicaCount: 4
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: vtiles.openhistoricalmap.org
    env:
      TILER_SERVER_PORT: 9090
      TILER_CACHE_TYPE: s3
      TILER_CACHE_BASEPATH: /mnt/data
      TILER_CACHE_MAX_ZOOM: 22
      # in case s3
      TILER_CACHE_REGION: us-east-1
      TILER_CACHE_BUCKET: tiler-cache-production
      TILER_CACHE_AWS_ACCESS_KEY_ID: {{PRODUCTION_TILER_CACHE_AWS_ACCESS_KEY_ID}}
      TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{PRODUCTION_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
      TILER_CACHE_AWS_ENDPOINT: ""
      EXECUTE_REINDEX: false
      EXECUTE_VACUUM_ANALYZE: false
    # In case you use TILER_CACHE_TYPE: file with  persistenceDisk 
    persistenceDisk:
      enabled: false
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
      # In case cloudProvider: aws
      # AWS_ElasticBlockStore_volumeID : {{PRODUCTION_TILER_SERVER_EBS}}
      # AWS_ElasticBlockStore_size: 100Gi
    resources:
      enabled: false
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
    autoscaling:
      enabled: true
      minReplicas: 4
      maxReplicas: 6
      cpuUtilization: 60  
  # ====================================================================================================
  # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
  # ====================================================================================================
  tilerServerCacheCleaner:
    enabled: false
    priorityClass: 'medium-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    replicaCount: 1
    command: './cache_cleaner.sh'
    resources:
      enabled: true
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "4"
    env:
      KILL_PROCESS: manually
      MAX_NUM_PS: 5
      PROCESS_NAME: tegola
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 1
      cpuUtilization: 90

  # ====================================================================================================
  # Variables for Tasking Manager DB
  # ====================================================================================================
  tmDb:
    enabled: true
    priorityClass: 'medium-priority'
    image:
      name: "postgis/postgis"
      tag: "11-2.5"
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    env:
      POSTGRES_DB: tm
      POSTGRES_PASSWORD: {{PRODUCTION_TM_DB_PASSWORD}}
      POSTGRES_USER: postgres
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-d
      AWS_ElasticBlockStore_volumeID: vol-03a2f95687a51a531
      AWS_ElasticBlockStore_size: 20Gi
    resources:
      enabled: false
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "2"
  # ====================================================================================================
  # Variables for Tasking Manager API
  # ====================================================================================================

  tmApi:
    enabled: true
    priorityClass: 'medium-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    replicaCount: 2
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '300'
    ingressDomain: tm-api.openhistoricalmap.org
    env:
      TM_ORG_NAME: OpenHistoricalMap
      TM_ORG_CODE: OHM
      TM_ORG_URL: www.openhistoricalmap.org
      TM_ORG_PRIVACY_POLICY_URL: www.openhistoricalmap.org/copyright
      TM_ORG_GITHUB: github.com/openhistoricalmap
      OSM_SERVER_URL: https://www.openhistoricalmap.org
      OSM_NOMINATIM_SERVER_URL: https://nominatim-api.openhistoricalmap.org
      OSM_REGISTER_URL: https://www.openhistoricalmap.org/user/new
      ID_EDITOR_URL: https://www.openhistoricalmap.org/edit?editor=id
      POTLATCH2_EDITOR_URL: https://www.openhistoricalmap.org/edit?editor=potlatch2
      TM_SECRET: {{PRODUCTION_TM_API_SECRET}}
      TM_CONSUMER_KEY: {{PRODUCTION_TM_API_CONSUMER_KEY}}
      TM_CONSUMER_SECRET: {{PRODUCTION_TM_API_CONSUMER_SECRET}}
      TM_EMAIL_FROM_ADDRESS: ohm-admins@googlegroups.com
      TM_EMAIL_CONTACT_ADDRESS: ohm-admins@googlegroups.com
      TM_SMTP_HOST: email-smtp.us-east-1.amazonaws.com
      TM_SMTP_PORT: 25
      TM_SMTP_USER: {{MAILER_USERNAME}}
      TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
      TM_DEFAULT_LOCALE: en
      TM_APP_API_URL: https://tm-api.openhistoricalmap.org
      TM_APP_BASE_URL: https://tasks.openhistoricalmap.org
      TM_IMPORT_MAX_FILESIZE: 3000000
      TM_MAX_AOI_AREA: 15000
      TM_APP_API_VERSION: v2
      # The following environment variables are for future versions of TM
      TM_CLIENT_ID: {{PRODUCTION_TM_CLIENT_ID}}
      TM_CLIENT_SECRET: {{PRODUCTION_TM_CLIENT_SECRET}}
      TM_DEFAULT_CHANGESET_COMMENT: production
      TM_REDIRECT_URI: https://tm-api.staging.openhistoricalmap.org/authorized
      TM_SCOPE: 'read_prefs write_api'
      # Add extra info
      TM_ORG_FB: https://www.facebook.com/openhistoricalmap
      TM_ORG_INSTAGRAM: https://www.openhistoricalmap.org
      TM_ORG_TWITTER: https://x.com/OpenHistMap
      TM_ORG_YOUTUBE: https://www.youtube.com/playlist?list=PLOi35w6_Hpx_CYdYBUpPeuiJ1djn5-wIx
    resources:
      enabled: false
      requests:
        memory: 1Gi
        cpu: '2'
      limits:
        memory: 2Gi
        cpu: '2'

  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
  nominatimApi:
    enabled: true
    priorityClass: 'medium-priority'
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: nominatim-api.openhistoricalmap.org
    replicaCount: 1
    env:
      PBF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-230727_1030.osm.pbf
      REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
      REPLICATION_UPDATE_INTERVAL: 60
      REPLICATION_RECHECK_INTERVAL: 30
      FREEZE: false
      IMPORT_WIKIPEDIA: false
      IMPORT_US_POSTCODES: false
      IMPORT_GB_POSTCODES: false
      IMPORT_TIGER_ADDRESSES: false
      THREADS: 8
      NOMINATIM_PASSWORD: {{PRODUCTION_NOMINATIM_PG_PASSWORD}}
      PGDATA: /var/lib/postgresql/16/main
      NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
      UPDATE_MODE: continuous
      OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
      IMPORT_STYLE: extratags
      EXTRA_TAGS: start_date,start_date:edtf,end_date,end_date:edt
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/16/main
      subPath: nominatim-pgdata
      AWS_ElasticBlockStore_volumeID: vol-012355ce0247b5ccc
      AWS_ElasticBlockStore_size: 50Gi
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large

# ====================================================================================================
# Variables for overpass-api
# ====================================================================================================
  overpassApi:
    enabled: true
    priorityClass: 'medium-priority'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: overpass-api.openhistoricalmap.org
    env:
      OVERPASS_META: 'attic'
      OVERPASS_MODE: 'init'
      OVERPASS_PLANET_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240715_0003.osm.pbf
      OVERPASS_DIFF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute
      OVERPASS_RULES_LOAD: '10'
      OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
      OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1484000'
      OVERPASS_ALLOW_DUPLICATE_QUERIES: 'yes'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      AWS_ElasticBlockStore_volumeID: vol-01d9060df5d14a903
      AWS_ElasticBlockStore_size: 100Gi
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
# ====================================================================================================
# Variables for taginfo
# ====================================================================================================
  taginfo:
    enabled: true
    priorityClass: 'medium-priority'
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: taginfo.openhistoricalmap.org
    env:
      URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
      URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
      URL_PLANET_FILE: 'none'
      URL_HISTORY_PLANET_FILE: 'none'
      TIME_UPDATE_INTERVAL: 7d
      OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/main/images/taginfo/taginfo-config-production.json
      TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
      DOWNLOAD_DB: 'languages wiki'
      CREATE_DB: 'db projects chronology'
      ENVIRONMENT: production
      AWS_S3_BUCKET: taginfo
      INTERVAL_DOWNLOAD_DATA: 7d
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
    cronjob:
      enabled: true
      schedule: "0 2 */3 * *"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
      resources:
        enabled: true
        requests:
          memory: "13Gi"
          cpu: "3600m"
        limits:
          memory: "14Gi"
          cpu: "3800m"
# ====================================================================================================
# Variables for osm-simple-metrics
# ====================================================================================================
  osmSimpleMetrics:
    enabled: true
    priorityClass: 'medium-priority'
    schedule: '0 2 * * *'
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
      
  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
  monitoringReplication:
    enabled: true
    priorityClass: 'medium-priority'
    schedule: '*/30 * * * *'
    env:
      CREATE_MISSING_FILES: "empty"
      REPLICATION_SEQUENCE_NUMBER: "000000"
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
  # ====================================================================================================
  # Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  changesetReplicationJob:
    enabled: true
    priorityClass: 'medium-priority'
    resources:
      enabled: false
      requests:
        memory: '20Gi'
        cpu: '8'
      limits:
        memory: '24Gi'
        cpu: '10'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large

# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
  osmchaWeb:
    enabled: true
    priorityClass: 'medium-priority'
    image:
      name: ghcr.io/openhistoricalmap/osmcha-frontend
      tag: 80eb324
# ====================================================================================================
# Variables for osmcha Api
# ====================================================================================================
  osmchaApi:
    enabled: true
    priorityClass: 'medium-priority'
    image:
      name: ghcr.io/openhistoricalmap/osmcha-django
      tag: 1e9d9dd
    ingressDomain: osmcha.openhistoricalmap.org
    env:
      DJANGO_SETTINGS_MODULE: "config.settings.production"
      OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
      DJANGO_SECRET_KEY: {{PRODUCTION_OSMCHA_DJANGO_SECRET_KEY}}
      DJANGO_SECURE_SSL_REDIRECT: "False"
      OSM_SERVER_URL: https://www.openhistoricalmap.org
      OAUTH_REDIRECT_URI: https://osmcha.openhistoricalmap.org/authorized
      OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
      ## frontend
      OSMCHA_URL: https://osmcha.openhistoricalmap.org
      OSMCHA_API_URL: www.openhistoricalmap.org
      OAUTH2_OSM_KEY: 0KWrkIIQtFBc4k_Blb2WuIi108w10GYXFwW30qvk-W8
      OAUTH2_OSM_SECRET: UAnkzA3FqCP6ZYhqN1NEyQRdG36ALv46XGI7UtHJ0_o
    resources:
      enabled: false
      requests:
        memory: "512Mi"
        cpu: "1"
      limits:
        memory: "512Mi"
        cpu: "1"
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
# ====================================================================================================
# Variables for osmcha osm-adiff-service
# ====================================================================================================
    adiffService:
      enabled: false
      priorityClass: 'medium-priority'
      image:
        name: ghcr.io/openhistoricalmap/osm-adiff-service
        tag: a343422497bb2ad4735ca7eada921b60eeb19e20
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
  osmchaDb:
    enabled: true
    priorityClass: 'medium-priority'
    image:
      name: "developmentseed/osmseed-osmcha-db"
      tag: "0.1.0-n767.h0090e97"
    env:
      POSTGRES_DB: osmcha
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: {{PRODUCTION_OSMCHA_PG_PASSWORD}}
    resources:
      enabled: false
      requests:
        memory: "20Gi"
        cpu: "8"
      limits:
        memory: "24Gi"
        cpu: "10"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      AWS_ElasticBlockStore_volumeID: vol-03f47ec4b4532e93d
      AWS_ElasticBlockStore_size: 20Gi
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large
# ====================================================================================================
# Planet files server
# ====================================================================================================
  planetFiles:
    enabled: false
    priorityClass: 'low-priority'

# ====================================================================================================
# Tiles cache SQS processor
# ====================================================================================================
ohm:
  tilerCache:
    enabled: false
    
  tilerCachePurge:
    enabled: false
    env:
      REGION_NAME: us-east-1
      NAMESPACE: default # Namespace to run the job
      SQS_QUEUE_URL: {{PRODUCTION_SQS_QUEUE_URL}}
      NODEGROUP_TYPE: misc_large # Nodegroup type to run the purge and seed job
      # Maximum number of active jobs in high concurrency queue
      MAX_ACTIVE_JOBS: 10 
      DELETE_OLD_JOBS_AGE: 3600 # 1 hours
      ## Execute purging
      EXECUTE_PURGE: true
      PURGE_CONCURRENCY: 64
      PURGE_MIN_ZOOM: 3
      PURGE_MAX_ZOOM: 10 # Purging zoom 15,16,17,18,19,20 takes hours to complete,we are going to remove direct from s3 the tiles for zoom 19-20
      ## Execute seeding
      EXECUTE_SEED: false
      SEED_CONCURRENCY: 64
      SEED_MIN_ZOOM: 0
      SEED_MAX_ZOOM: 8
      ## Remove tiles from s3 for zoom levels
      ZOOM_LEVELS_TO_DELETE: 11,12,13,14,15,16,17,18,19,20
      S3_BUCKET_CACHE_TILER: tiler-cache-production
      S3_BUCKET_PATH_FILES: mnt/data/osm
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large

  tilerCacheSeedGlobal:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 128
      MIN_ZOOM: 0
      MAX_ZOOM: 7
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large

  tilerCacheSeedCoverage:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 128
      TILE_LIST_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/tile_coverage/tiles.list
      MIN_ZOOM: 8
      MAX_ZOOM: 14
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: misc_large