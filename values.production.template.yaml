osm-seed:

  # ====================================================================================================
  # ====================================================================================================
  # ==================================Global Configurations=============================================
  # ====================================================================================================
  # ====================================================================================================
  environment: production
  # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
  cloudProvider: aws

  # ====================================================================================================
  # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
  # ====================================================================================================
  AWS_S3_BUCKET: {{PRODUCTION_S3_BUCKET}}

  # AWS SSL ARN
  AWS_SSL_ARN: {{AWS_SSL_ARN}}

  # Specify serviceType.
  #
  # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
  # Use `NodePort` for local testing on minikube.
  #
  # The recommended setting is `ClusterIP`, and then following the instructions to
  # point a DNS record to the cluster IP address. This will setup the ingress rules
  # for all services as subdomains and configure SSL using Lets Encrypt.
  #
  # If you specify `LoadBalancer` as the service type, if you also specify
  # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
  # as the SSL certificate for your services. Else, you will need to configure
  # SSL separately. 
  serviceType: ClusterIP

  # Domain that is pointed to the clusterIP
  # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
  # Then, the cluster configuration will setup services at their respective subdomains:
  # - web.osmseed.example.com
  # - overpass.osmseed.example.com
  # - nominatim.osmseed.example.com
  # - etc.
  domain: openhistoricalmap.org

  # ====================================================================================================
  # Configuration for Lets Encrypt setup
  # ====================================================================================================

  # Admin Email address used when generating Lets Encrypt certificates.
  # You will be notified of expirations, etc. on this email address.
  adminEmail: ohm-admins@googlegroups.com

  # ====================================================================================================
  # Variables for osm-seed database
  # ====================================================================================================
  db:
    enabled: true
    # For node selector you should create the node with a label "nodegroup_type"
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: db
    env:
      POSTGRES_DB: {{PRODUCTION_DB}}
      POSTGRES_USER: {{PRODUCTION_DB_USER}}
      POSTGRES_PASSWORD: {{PRODUCTION_DB_PASSWORD}}
      LOG_STATEMENT: "none"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-db
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID : vol-08e31c1930e00a478
      AWS_ElasticBlockStore_size: 600Gi
    resources:
      enabled: false
      requests:
        memory: "10Gi"
        cpu: "5"
      limits:
        memory: "10Gi"
        cpu: "5"
    sharedMemorySize: 512Mi

  # ====================================================================================================
  # Variables for osm-seed website
  # ====================================================================================================
  web:
    enabled: true
    replicaCount: 2
    # Set staticIp, if you are using cloudProvider=gcp
    staticIp: c
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: www.openhistoricalmap.org
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    env:
      MAILER_ADDRESS: {{MAILER_ADDRESS}}
      MAILER_DOMAIN: openhistoricalmap.org
      MAILER_USERNAME: {{MAILER_USERNAME}}
      MAILER_PASSWORD: {{MAILER_PASSWORD}}
      OSM_id_key: {{PRODUCTION_ID_APPLICATION}}
      OAUTH_CLIENT_ID: {{PRODUCTION_OAUTH_CLIENT_ID}}
      OAUTH_KEY: {{PRODUCTION_OAUTH_KEY}}
      MAILER_FROM: ohm-admins@googlegroups.com
      NOMINATIM_URL: nominatim-api.openhistoricalmap.org
      OVERPASS_URL: overpass-api.openhistoricalmap.org
      NEW_RELIC_LICENSE_KEY: {{PRODUCTION_NEW_RELIC_LICENSE_KEY}}
      NEW_RELIC_APP_NAME: {{PRODUCTION_NEW_RELIC_APP_NAME}}
      ORGANIZATION_NAME: OpenHistoricalMap
      WEBSITE_STATUS: "online"
    resources:
      enabled: true
      requests:
        memory: "3Gi"
        cpu: "2"
      limits:
        memory: "3Gi"
        cpu: "2"
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      cpuUtilization: 80
    sharedMemorySize: 512Mi
    livenessProbeExec: true
  # ====================================================================================================
  # Variables for memcached. Memcached is used to store session cookies
  # ====================================================================================================
  memcached:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    resources:
      enabled: false
      requests:
        memory: "8Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "2"

  # ====================================================================================================
  # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
  # ====================================================================================================
  planetDump:
    enabled: true
    schedule: '0 0 * * *'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: job
    env:
      OVERWRITE_PLANET_FILE: false
    resources:
      enabled: false
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for full-history container
  # ====================================================================================================
  fullHistory:
    enabled: true
    schedule: '0 0 * * 0'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: job
    env:
      OVERWRITE_FHISTORY_FILE: false
    resources:
      enabled: false
      requests:
        memory: "4Gi"
        cpu: "2"
      limits:
        memory: "8Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for id-editor
  # ====================================================================================================
  idEditor:
    enabled: false
    replicaCount: 1
    # Set staticIp, if you are using cloudProvider=gcp
    staticIp: 35.233.232.82
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    env:
      ID_EDITOR_PORT: 80
      OSM_API_PROTOCOL: http
      OSM_API_DOMAIN: aaee232301c4644ff971899b35406d18-998593153.us-east-1.elb.amazonaws.com
      OAUTH_CONSUMER_KEY: LQCgtGKQ0MqTLnxxkQ0MZZz8NQmHjAcA21o7Thua
      OAUTH_SECRET: YF2df1It12j1yFYPAdZgtafzVW77L3R35qY34QDp
    resources:
      enabled: false
      requests:
        memory: "300Mi"
        cpu: "0.4"
      limits:
        memory: "400Mi"
        cpu: "0.5"
  # ====================================================================================================
  # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  replicationJob:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    resources:
      enabled: false
      requests:
        memory: "20Gi"
        cpu: "8"
      limits:
        memory: "24Gi"
        cpu: "10"

  # ====================================================================================================
  # Variables for osm-seed to pupulate the apidb
  # ====================================================================================================
  populateApidb:
    enabled: false
    env:
      URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
    resources:
      enabled: false
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "2.5"

  # ====================================================================================================
  # Variables to start a pod to process osm files
  # ====================================================================================================
  osmProcessor:
    enabled: false
    env: 
      URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
      OSM_FILE_ACTION: simple_pbf
    resources:
      enabled: false
      requests:
        memory: "14Gi"
        cpu: "4"
      limits:
        memory: "16Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for restoring the DB
  # ====================================================================================================

  dbBackupRestore:
    cronjobs:
    - name: web-db
      enabled: true
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: true
        BACKUP_CLOUD_FOLDER: database/web-api-db
        BACKUP_CLOUD_FILE: ohm-api-web-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
    - name: osmcha-db
      enabled: false
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: 'true'
        BACKUP_CLOUD_FOLDER: database/osmcha-db
        BACKUP_CLOUD_FILE: osmseed-osmcha-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for tiler-db
  # ====================================================================================================

  tilerDb:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: db
    env:
      POSTGRES_HOST: production-tiler-db
      POSTGRES_DB: tiler-osm
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: {{PRODUCTION_TILER_DB_PASSWORD}}
      POSTGRES_PORT: 5432
      # for 20GB
      POSTGRES_DB_MAX_CONNECTIONS: 500
      POSTGRES_DB_SHARED_BUFFERS: 5GB
      POSTGRES_DB_WORK_MEM: 15MB
      POSTGRES_DB_MAINTENANCE_WORK_MEM: 1GB
      POSTGRES_DB_EFFECTIVE_CACHE_SIZE: 10GB
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-d
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID : vol-05d2e4b9c40c7dbdd
      AWS_ElasticBlockStore_size: 400Gi
    resources:
      enabled: true
      requests:
        memory: "20Gi"
        cpu: "4"
      limits:
        memory: "20Gi"
        cpu: "4"
  # ====================================================================================================
  # Variables for tiler-imposm
  # ====================================================================================================

  tilerImposm:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    env:
      TILER_IMPORT_FROM: osm
      TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240528_0003.osm.pbf
      REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
      SEQUENCE_NUMBER: "1420000"
      OVERWRITE_STATE: false
      UPLOAD_EXPIRED_FILES: true
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID: vol-098a31471b9631a5e
      AWS_ElasticBlockStore_size: 200Gi
    resources:
      enabled: false
      requests:
        memory: "16Gi"
        cpu: "2"
      limits:
        memory: "24Gi"
        cpu: "3"
  # ====================================================================================================
  # Variables for tiler-server
  # ====================================================================================================

  tilerServer:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    replicaCount: 2
    commad: './start.sh'
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: vtiles.openhistoricalmap.org
    env:
      TILER_SERVER_PORT: 9090
      TILER_CACHE_TYPE: s3
      TILER_CACHE_BASEPATH: /mnt/data
      TILER_CACHE_MAX_ZOOM: 22
      # in case s3
      TILER_CACHE_REGION: us-east-1
      TILER_CACHE_BUCKET: tiler-cache-production
      TILER_CACHE_AWS_ACCESS_KEY_ID: {{PRODUCTION_TILER_CACHE_AWS_ACCESS_KEY_ID}}
      TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{PRODUCTION_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
    # In case you use TILER_CACHE_TYPE: file with  persistenceDisk 
    persistenceDisk:
      enabled: false
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
      # In case cloudProvider: aws
      AWS_ElasticBlockStore_volumeID : {{PRODUCTION_TILER_SERVER_EBS}}
      AWS_ElasticBlockStore_size: 100Gi
    resources:
      enabled: true
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "10Gi"
        cpu: "2"
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 2
      cpuUtilization: 60  
  # ====================================================================================================
  # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
  # ====================================================================================================
  tilerServerCacheCleaner:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    replicaCount: 1
    command: './cache_cleaner.sh'
    resources:
      enabled: true
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "4"
    env:
      KILL_PROCESS: manually
      MAX_NUM_PS: 5
      PROCESS_NAME: tegola
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 1
      cpuUtilization: 90
  # ====================================================================================================
  # Variables for tiler-visor
  # ====================================================================================================
  tilerVisor:
    enabled: false
    replicaCount: 1
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    # Set staticIp, if you are using cloudProvider=gcp
    staticIp: 35.233.232.82
    env:
      TILER_VISOR_PROTOCOL: http
      TILER_VISOR_PORT: 8081
    resources:
      enabled: false
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "2"

  # ====================================================================================================
  # Variables for Tasking Manager DB
  # ====================================================================================================
  tmDb:
  enabled: false
  image:
    name: "postgres"
    tag: "11"
  nodeSelector:
    enabled: false
    label_key: nodegroup_type
    label_value: web
  env:
    POSTGRES_DB: tm
    POSTGRES_PASSWORD: {{PRODUCTION_TM_DB_PASSWORD}}
    POSTGRES_USER: postgres
  resources:
    enabled: false
    requests:
      memory: "1Gi"
      cpu: "2"
    limits:
      memory: "2Gi"
      cpu: "2"

  # ====================================================================================================
  # Variables for Tasking Manager API
  # ====================================================================================================

  tmApi:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    replicaCount: 2
    staticIp: c
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: tm-api.openhistoricalmap.org
    env:
      POSTGRES_HOST: {{PRODUCTION_TM_API_DB_HOST}}
      POSTGRES_DB: {{PRODUCTION_TM_API_DB}}
      POSTGRES_PASSWORD: {{PRODUCTION_TM_API_DB_PASSWORD}}
      POSTGRES_USER: {{PRODUCTION_TM_API_DB_USER}}
      POSTGRES_PORT: 5432
      TM_ORG_NAME: 'OpenHistoricalMap'
      TM_ORG_CODE: 'OHM'
      TM_ORG_URL: 'www.openhistoricalmap.org'
      TM_ORG_PRIVACY_POLICY_URL: 'www.openhistoricalmap.org/copyright'
      TM_ORG_GITHUB: 'github.com/openhistoricalmap'
      OSM_SERVER_URL: 'https://www.openhistoricalmap.org'
      OSM_NOMINATIM_SERVER_URL: 'https://nominatim-api.openhistoricalmap.org'
      OSM_REGISTER_URL: 'https://www.openhistoricalmap.org/user/new'
      ID_EDITOR_URL: 'https://www.openhistoricalmap.org/edit?editor=id'
      POTLATCH2_EDITOR_URL: 'https://www.openhistoricalmap.org/edit?editor=potlatch2'
      TM_SECRET: {{PRODUCTION_TM_API_SECRET}}
      TM_CONSUMER_KEY: {{PRODUCTION_TM_API_CONSUMER_KEY}}
      TM_CONSUMER_SECRET: {{PRODUCTION_TM_API_CONSUMER_SECRET}}
      TM_EMAIL_FROM_ADDRESS: 'ohm-admins@googlegroups.com'
      TM_SMTP_HOST: 'email-smtp.us-east-1.amazonaws.com'
      TM_SMTP_PORT: 25
      TM_SMTP_USER: {{MAILER_USERNAME}}
      TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
      TM_DEFAULT_LOCALE: 'en'
      TM_APP_API_URL: 'https://tm-api.openhistoricalmap.org'
      TM_APP_BASE_URL: 'https://tasks.openhistoricalmap.org'
      TM_IMPORT_MAX_FILESIZE: 3000000
      TM_MAX_AOI_AREA: 15000
    resources:
      enabled: false
      requests:
        memory: "1Gi"
        cpu: "2"
      limits:
        memory: "2Gi"
        cpu: "2"


  
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
  nominatimApi:
    enabled: true
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: nominatim-api.openhistoricalmap.org
    replicaCount: 1
    env:
      PBF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-230727_1030.osm.pbf
      REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
      REPLICATION_UPDATE_INTERVAL: 60
      REPLICATION_RECHECK_INTERVAL: 30
      FREEZE: false
      IMPORT_WIKIPEDIA: false
      IMPORT_US_POSTCODES: false
      IMPORT_GB_POSTCODES: false
      IMPORT_TIGER_ADDRESSES: false
      THREADS: 8
      NOMINATIM_PASSWORD: {{PRODUCTION_NOMINATIM_PG_PASSWORD}}
      PGDATA: /var/lib/postgresql/14/main
      NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
      UPDATE_MODE: continuous
      OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/14/main
      subPath: nominatim-pgdata
      # Minikube
      localVolumeHostPath: /mnt/nominatim-db-data
      localVolumeSize: 20Gi
      # AWS
      AWS_ElasticBlockStore_volumeID: {{PRODUCTION_NOMINATIM_DB_EBS}}
      AWS_ElasticBlockStore_size: 100Gi
      # GCP
      GCP_gcePersistentDisk_pdName: osmseed-disk-nominatim_db-v1
      GCP_gcePersistentDisk_size: 50Gi
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web

# ====================================================================================================
# Variables for overpass-api
# ====================================================================================================
  overpassApi:
    enabled: true
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: overpass-api.openhistoricalmap.org
    env:
      OVERPASS_META: 'attic'
      OVERPASS_MODE: 'init'
      OVERPASS_PLANET_URL: 'https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240102_0000.osm.pbf'
      OVERPASS_DIFF_URL: 'http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute'
      OVERPASS_RULES_LOAD: '10'
      OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
      OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1258000'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      AWS_ElasticBlockStore_volumeID: vol-0fc60d3338a1e77c8
      AWS_ElasticBlockStore_size: 100Gi
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
# ====================================================================================================
# Variables for taginfo
# ====================================================================================================
  taginfo:
    enabled: true
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: taginfo.openhistoricalmap.org
    env:
      URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
      URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
      URL_PLANET_FILE: 'none'
      URL_HISTORY_PLANET_FILE: 'none'
      TIME_UPDATE_INTERVAL: 7d
      OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/main/images/taginfo/taginfo-config-production.json
      TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
      DOWNLOAD_DB: 'languages wiki'
      CREATE_DB: 'db projects chronology'
      ENVIRONMENT: production
      AWS_S3_BUCKET: taginfo
      INTERVAL_DOWNLOAD_DATA: 7d
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
    cronjob:
      enabled: false
      schedule: "0 2 */3 * *"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge 
# ====================================================================================================
# Variables for osm-simple-metrics
# ====================================================================================================
  osmSimpleMetrics:
    enabled: true
    schedule: '0 2 * * *'
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: job
      
  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
  monitoringReplication:
    enabled: true
    schedule: '*/30 * * * *'
    env:
      CREATE_MISSING_FILES: "empty"
      REPLICATION_SEQUENCE_NUMBER: "000000"
    resources:
      enabled: false
      requests:
        memory: '1Gi'
        cpu: '2'
      limits:
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
  # ====================================================================================================
  # Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  changesetReplicationJob:
    enabled: true
    resources:
      enabled: false
      requests:
        memory: '20Gi'
        cpu: '8'
      limits:
        memory: '24Gi'
        cpu: '10'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web

# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
  osmchaWeb:
    enabled: true
# ====================================================================================================
# Variables for osmcha Api
# ====================================================================================================
  osmchaApi:
    enabled: true
    image:
      name: "ghcr.io/openhistoricalmap/osmcha-django"
      tag: "a1bcea85dc1f7c27566c20bafe7fff7aaa1e38a4"
    ingressDomain: osmcha.openhistoricalmap.org
    env:
      DJANGO_SETTINGS_MODULE: "config.settings.production"
      OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
      DJANGO_SECRET_KEY: {{PRODUCTION_OSMCHA_DJANGO_SECRET_KEY}}
      OAUTH_OSM_KEY: {{PRODUCTION_OSMCHA_API_CONSUMER_KEY}}
      OAUTH_OSM_SECRET: {{PRODUCTION_OSMCHA_API_CONSUMER_SECRET}}
      DJANGO_SECURE_SSL_REDIRECT: "False"
      OSM_SERVER_URL: https://www.openhistoricalmap.org
      OAUTH_REDIRECT_URI: https://osmcha.openhistoricalmap.org/oauth-landing.html
      OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
      ## frontend
      OSMCHA_URL: https://osmcha.openhistoricalmap.org
      OSMCHA_API_URL: www.openhistoricalmap.org
      REACT_APP_OSM_URL: https://www.openhistoricalmap.org
      REACT_APP_OSM_API: https://www.openhistoricalmap.org/api/0.6
      REACT_APP_OVERPASS_BASE: //overpass-api.openhistoricalmap.org/api/interpreter
      REACT_APP_ENABLE_REAL_CHANGESETS: 0
      REACT_APP_MAPBOX_ACCESS_TOKEN: {{PRODUCTION_OSMCHA_REACT_APP_MAPBOX_ACCESS_TOKEN}}
    resources:
      enabled: false
      requests:
        memory: "512Mi"
        cpu: "1"
      limits:
        memory: "512Mi"
        cpu: "1"
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
  osmchaDb:
    enabled: true
    image:
      name: "developmentseed/osmseed-osmcha-db"
      tag: "0.1.0-n767.h0090e97"
    env:
      POSTGRES_DB: {{PRODUCTION_OSMCHA_PG_DATABASE}}
      POSTGRES_USER: {{PRODUCTION_OSMCHA_PG_USER}}
      POSTGRES_PASSWORD: {{PRODUCTION_OSMCHA_PG_PASSWORD}}
    resources:
      enabled: false
      requests:
        memory: "20Gi"
        cpu: "8"
      limits:
        memory: "24Gi"
        cpu: "10"
    persistenceDisk:
      enabled: false
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      AWS_ElasticBlockStore_volumeID: vol-065901d9a34a6fbf9
      AWS_ElasticBlockStore_size: 100Gi
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web
# ====================================================================================================
# Planet files server
# ====================================================================================================
  planetFiles:
    enabled: false
    