osm-seed:

  # ====================================================================================================
  # ====================================================================================================
  # ==================================Global Configurations=============================================
  # ====================================================================================================
  # ====================================================================================================
  environment: production
  # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
  cloudProvider: aws

  # ====================================================================================================
  # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
  # ====================================================================================================
  AWS_S3_BUCKET: {{PRODUCTION_S3_BUCKET}}

  # AWS SSL ARN
  AWS_SSL_ARN: {{PRODUCTION_AWS_SSL_ARN}}

  # Specify serviceType.
  #
  # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
  # Use `NodePort` for local testing on minikube.
  #
  # The recommended setting is `ClusterIP`, and then following the instructions to
  # point a DNS record to the cluster IP address. This will setup the ingress rules
  # for all services as subdomains and configure SSL using Lets Encrypt.
  #
  # If you specify `LoadBalancer` as the service type, if you also specify
  # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
  # as the SSL certificate for your services. Else, you will need to configure
  # SSL separately. 
  serviceType: ClusterIP
  ingressClassNameType: "alb" # ALB works with ACM
  ingressClassName: alb
  alb:
    certificateArn: {{PRODUCTION_AWS_SSL_ARN}}
    enableWaf:
      enabled: true
      wafAclArn: {{PRODUCTION_AWS_WAF_WEBACL_ARN}}
  # Domain that is pointed to the clusterIP
  # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
  # Then, the cluster configuration will setup services at their respective subdomains:
  # - web.osmseed.example.com
  # - overpass.osmseed.example.com
  # - nominatim.osmseed.example.com
  # - etc.
  domain: openhistoricalmap.org

  # ====================================================================================================
  # Configuration for Lets Encrypt setup
  # ====================================================================================================

  # Admin Email address used when generating Lets Encrypt certificates.
  # You will be notified of expirations, etc. on this email address.
  adminEmail: admin@openhistoricalmap.org

  # ====================================================================================================
  # Variables for osm-seed database
  # ====================================================================================================
  db:
    enabled: true
    priorityClass: high-priority
    env:
      POSTGRES_DB: {{PRODUCTION_DB}}
      POSTGRES_USER: {{PRODUCTION_DB_USER}}
      POSTGRES_PASSWORD: {{PRODUCTION_DB_PASSWORD}}
      LOG_STATEMENT: "none"
      PGDATA: /var/lib/postgresql/data/pgdata
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-db
      AWS_ElasticBlockStore_volumeID : vol-0595bcb6ac2ba84a3
      AWS_ElasticBlockStore_size: 600Gi
    resources:
      enabled: true
      requests:
        enabled: true
        memory: "14Gi"
        cpu: "1000m"
      limits:
        enabled: false
        memory: "6Gi"
        cpu: "1700m"
    sharedMemorySize: 2Gi
    postgresqlConfig:
      enabled: true
      values: |
        listen_addresses = '*'
        max_connections = 200
        shared_buffers = 4GB
        work_mem = 20MB
        maintenance_work_mem = 512MB
        dynamic_shared_memory_type = posix
        effective_io_concurrency = 200
        max_wal_size = 1GB
        min_wal_size = 256MB
        random_page_cost = 1.0
        effective_cache_size = 8GB
        log_min_duration_statement = 3000
        log_connections = on
        log_disconnections = on
        log_duration = off
        log_lock_waits = on
        log_statement = 'none'
        log_timezone = 'Etc/UTC'
        datestyle = 'iso, mdy'
        timezone = 'Etc/UTC'
        lc_messages = 'en_US.utf8'
        lc_monetary = 'en_US.utf8'
        lc_numeric = 'en_US.utf8'
        lc_time = 'en_US.utf8'
        default_text_search_config = 'pg_catalog.english'
        # Parallelism settings
        max_parallel_workers_per_gather = 2
        max_parallel_workers = 4
        max_worker_processes = 4
        parallel_tuple_cost = 0.05
        parallel_setup_cost = 500
        min_parallel_table_scan_size = 2MB
        min_parallel_index_scan_size = 256kB
        session_preload_libraries = 'auto_explain'
        auto_explain.log_min_duration = '3s'
        shared_preload_libraries = 'osm-logical'
        wal_level=logical
        max_replication_slots=1
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: api_db_xlarge
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["database"]

  # ====================================================================================================
  # Variables for osm-seed website
  # ====================================================================================================
  web:
    enabled: true
    priorityClass: high-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    replicaCount: 2
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: www.openhistoricalmap.org
    env:
      MAILER_ADDRESS: {{MAILER_ADDRESS}}
      MAILER_DOMAIN: openhistoricalmap.org
      MAILER_USERNAME: {{MAILER_USERNAME}}
      MAILER_PASSWORD: {{MAILER_PASSWORD}}
      OSM_id_key: {{PRODUCTION_ID_APPLICATION}}
      OAUTH_CLIENT_ID: {{PRODUCTION_OAUTH_CLIENT_ID}}
      OAUTH_KEY: {{PRODUCTION_OAUTH_KEY}}
      MAILER_FROM: web@noreply.penhistoricalmap.org
      NOMINATIM_URL: nominatim.openhistoricalmap.org
      OVERPASS_URL: overpass-api.openhistoricalmap.org
      NEW_RELIC_LICENSE_KEY: 'none'
      NEW_RELIC_APP_NAME: 'none'
      ORGANIZATION_NAME: OpenHistoricalMap
      WEBSITE_STATUS: 'online'
      RAILS_CREDENTIALS_YML_ENC: {{PRODUCTION_RAILS_CREDENTIALS_YML_ENC}}
      RAILS_MASTER_KEY: {{PRODUCTION_RAILS_MASTER_KEY}}
      RAILS_ENV: production
      RAILS_LOG_LEVEL: warn
      RAILS_STORAGE_SERVICE: s3
      RAILS_STORAGE_REGION: us-east-1
      RAILS_STORAGE_BUCKET: ohm-website-production
      EXTERNAL_CGIMAP: true
      PASSENGER_MAX_POOL_SIZE: 6
      OPENSTREETMAP_AUTH_ID: {{PRODUCTION_OPENSTREETMAP_AUTH_ID}}
      OPENSTREETMAP_AUTH_SECRET: {{PRODUCTION_OPENSTREETMAP_AUTH_SECRET}}
      WIKIPEDIA_AUTH_ID: {{PRODUCTION_WIKIPEDIA_AUTH_ID}}
      WIKIPEDIA_AUTH_SECRET: {{PRODUCTION_WIKIPEDIA_AUTH_SECRET}}
    resources:
      enabled: true
      requests:
        enabled: true
        memory: "2.5Gi"
        cpu: "1200m"
      limits:
        enabled: true
        memory: "3Gi"
        cpu: "1500m"
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 6
      cpuUtilization:
        enabled: true
        value: 80
      memoryUtilization:
        enabled: false
        value: 80
      behavior:
        scaleUp:
          stabilizationWindowSeconds: 60       # Wait 60 seconds of sustained high usage before scaling up.
          policyValue: 100                     # Allow up to 100% increase in replicas (e.g., 3 → 6).
          periodSeconds: 60                    # This increase limit applies within a 60-second window.
        scaleDown:
          stabilizationWindowSeconds: 120      # Wait 120 seconds (2 minutes) of low usage before scaling down.
          policyValue: 50                      # Allow up to 50% decrease in replicas at once (e.g., 6 → 3).
          periodSeconds: 60                    # This decrease limit applies within a 60-second window.
    sharedMemorySize: 512Mi
    livenessProbeExec: true
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web_medium
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
    podAntiAffinity: # To set that no two pods are scheduled on the same node
      enabled: true
  # ====================================================================================================
  # Variables for memcached. Memcached is used to store session cookies
  # ====================================================================================================
  memcached:
    enabled: true
    priorityClass: high-priority
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "8Gi"
        cpu: "2"
      limits:
        enabled: false
        memory: "8Gi"
        cpu: "2"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web_helpers_medium
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
    # ====================================================================================================
    # Cgimap
    # ====================================================================================================
  cgimap:
    enabled: true
    image:
      name: ghcr.io/openhistoricalmap/cgimap
      tag: 0.0.1-0.dev.git.2463.hb0e1e7f
    priorityClass: "high-priority"
    resources:
      enabled: true
      requests:
        enabled: true
        memory: "0.5Gi"
        cpu: "100m"
      limits:
        enabled: true
        memory: "1Gi"
        cpu: "300m"
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      cpuUtilization: 50
      memoryUtilization: 50
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web_helpers_medium
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
  # ====================================================================================================
  # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
  # ====================================================================================================
  planetDump:
    enabled: true
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    schedule: '0 3 * * *'
    env:
      OVERWRITE_PLANET_FILE: false
      PLANET_EXPORT_METHOD: planet-dump-ng
      DUMP_CLOUD_URL: s3://osmseed-production-db/database/web-api-db/state.txt
      PLANET_EPOCH_DATE: '1970-01-01'
      PLANET_DUMP_NG_METADATA_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/refs/heads/main/images/planet-dump/metadata.yml
    resources:
      enabled: true
      requests:
        enabled: true
        memory: "6Gi"
        cpu: "3500m"
      limits:
        enabled: false
        memory: "8Gi"
        cpu: "4000m"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: job
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["job"]
  # ====================================================================================================
  # Variables for full-history container
  # ====================================================================================================
  fullHistory:
    enabled: true
    priorityClass: low-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    schedule: '0 4 * * 0'
    env:
      OVERWRITE_FHISTORY_FILE: false
      PLANET_EXPORT_METHOD: planet-dump-ng
      DUMP_CLOUD_URL: s3://osmseed-production-db/database/web-api-db/state.txt
      PLANET_EPOCH_DATE: '1970-01-01'
      PLANET_DUMP_NG_METADATA_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/refs/heads/main/images/planet-dump/metadata.yml
    resources:
      enabled: true
      requests:
        enabled: true
        memory: "6Gi"
        cpu: "3500m"
      limits:
        enabled: false
        memory: "8Gi"
        cpu: "4000m"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: job_xlarge
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["job"]
  # ====================================================================================================
  # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  replicationJob:
    enabled: true
    priorityClass: high-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    env:
      ENABLE_SEND_SLACK_MESSAGE: "true"
      SLACK_WEBHOOK_URL: {{OHM_SLACK_WEBHOOK_URL}}
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "20Gi"
        cpu: "8"
      limits:
        enabled: false
        memory: "24Gi"
        cpu: "10"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web_helpers_medium
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
  # ====================================================================================================
  # Variables for osm-seed to pupulate the apidb
  # ====================================================================================================
  populateApidb:
    enabled: false
    priorityClass: low-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    env:
      URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "1Gi"
        cpu: "2"
      limits:
        enabled: false
        memory: "2Gi"
        cpu: "2.5"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web_helpers_medium
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
  # ====================================================================================================
  # Variables to start a pod to process osm files
  # ====================================================================================================
  osmProcessor:
    enabled: false
    priorityClass: low-priority
    env: 
      URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
      OSM_FILE_ACTION: simple_pbf
    resources:
      enabled: false
      requests:
        memory: "14Gi"
        cpu: "4"
      limits:
        memory: "16Gi"
        cpu: "4"

  # ====================================================================================================
  # Variables for restoring the DB
  # ====================================================================================================

  dbBackupRestore:
    cronjobs:
    - name: web-db
      enabled: true
      serviceAccount:
        enabled: true
        name: ohm-s3-database-backup-access-production
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: true
        BACKUP_CLOUD_FOLDER: database/web-api-db
        BACKUP_CLOUD_FILE: ohm-api-web-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: job_xlarge
      nodeAffinity:
        enabled: true
        key: "nodegroup_type"
        values: ["job"]
    - name: tm-db
      enabled: true
      serviceAccount:
        enabled: true
        name: ohm-s3-database-backup-access-production
      schedule: '0 1 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: true
        BACKUP_CLOUD_FOLDER: database/tm-db
        BACKUP_CLOUD_FILE: ohm-tm-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
      resources:
        enabled: false
        requests:
          memory: '300Mi'
          cpu: '0.5'
        limits:
          memory: '400Mi'
          cpu: '0.6'
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: job
      nodeAffinity:
        enabled: true
        key: "nodegroup_type"
        values: ["job"]
    - name: osmcha-db
      enabled: false
      serviceAccount:
        enabled: true
        name: ohm-s3-database-backup-access-production
      schedule: '0 0 * * *'
      env:
        # backup/restore
        DB_ACTION: backup
        # Naming backup files
        SET_DATE_AT_NAME: 'true'
        BACKUP_CLOUD_FOLDER: database/osmcha-db
        BACKUP_CLOUD_FILE: osmseed-osmcha-db
        AWS_S3_BUCKET: {{PRODUCTION_DB_BACKUP_S3_BUCKET}}
        # Clean up backups options
        CLEANUP_BACKUPS: true
        RETENTION_DAYS: '30'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
      nodeAffinity:
        enabled: false
        key: "nodegroup_type"
        values: ["job"]
  # ====================================================================================================
  # Variables for tiler-db
  # ====================================================================================================

  tilerDb:
    enabled: false
    priorityClass: medium-priority
    useExternalHost: # When we are using useExternalHost.enabled= true other variables are giong to be disable ans use the external host config
      enabled: true
    env:
      POSTGRES_HOST: {{PRODUCTION_TILER_DB_HOST}}
      POSTGRES_DB: tiler_osm_production
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: {{PRODUCTION_TILER_DB_PASSWORD}}
      POSTGRES_PORT: 5432
    sharedMemorySize: 2Gi
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-d
      AWS_ElasticBlockStore_volumeID: vol-0e4d738f35f5361fe
      AWS_ElasticBlockStore_size: 200Gi
    resources:
      enabled: true
      requests:
        enabled: false
        memory: "29Gi"
        cpu: "7500m"
      limits:
        enabled: false
        memory: "29Gi"
        cpu: "7600m"
    postgresqlConfig:
      enabled: true
      values: |
        listen_addresses = '*'
        max_connections = 100
        shared_buffers = 10GB
        work_mem = 256MB
        maintenance_work_mem = 2GB
        dynamic_shared_memory_type = posix
        effective_io_concurrency = 300
        max_wal_size = 4GB
        min_wal_size = 512MB
        random_page_cost = 1.0
        effective_cache_size = 24GB
        log_min_duration_statement = 15000
        log_connections = on
        log_disconnections = on
        log_duration = off
        log_lock_waits = on
        log_statement = 'none'
        log_timezone = 'Etc/UTC'
        datestyle = 'iso, mdy'
        timezone = 'Etc/UTC'
        lc_messages = 'en_US.utf8'
        lc_monetary = 'en_US.utf8'
        lc_numeric = 'en_US.utf8'
        lc_time = 'en_US.utf8'
        default_text_search_config = 'pg_catalog.english'

        # Parallelism settings
        max_parallel_workers_per_gather = 8
        max_parallel_workers = 16
        max_worker_processes = 16
        parallel_tuple_cost = 0.05
        parallel_setup_cost = 500
        min_parallel_table_scan_size = 8MB
        min_parallel_index_scan_size = 512kB

        # Enable auto_explain and pg_stat_statements
        shared_preload_libraries = 'auto_explain'
        auto_explain.log_min_duration = '10s'

        # Timeout settings
        tcp_keepalives_idle = 300
        tcp_keepalives_interval = 60
        tcp_keepalives_count = 10

        # Disable join options for routes
        enable_mergejoin = true
        enable_hashjoin = true

        # Timeout settings for queries
        statement_timeout = '600s'
        lock_timeout = '60s'
        idle_in_transaction_session_timeout = '300s'

        # pg_stat_statements settings
        pg_stat_statements.max = 15000
        pg_stat_statements.track = none
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: tiler_db
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["tiler_db"]
  # ====================================================================================================
  # Variables for tiler-imposm
  # ====================================================================================================

  tilerImposm:
    enabled: false
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    env:
      TILER_IMPORT_FROM: osm
      TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-250117_0002.osm.pbf
      REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
      SEQUENCE_NUMBER: "1690000"
      OVERWRITE_STATE: false
      UPLOAD_EXPIRED_FILES: true
      IMPORT_NATURAL_EARTH: true
      IMPORT_OSM_LAND: true
      IMPOSM3_IMPORT_LAYERS: "all"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
      AWS_ElasticBlockStore_volumeID: vol-02dfd0aac5821f25b
      AWS_ElasticBlockStore_size: 50Gi
    resources:
      enabled: true
      requests:
        enabled: false
        memory: "2Gi"
        cpu: "1"
      limits:
        enabled: false
        memory: "2Gi"
        cpu: "1"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: tiler_db
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["tiler_db"]
  # ====================================================================================================
  # Variables for tiler-server
  # ====================================================================================================

  tilerServer:
    enabled: false
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    externalService:
      enabled: false
      ip: {{PRODUCTION_TILER_SERVER_HOST}}
      port: 9090
    replicaCount: 4
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: vtiles.openhistoricalmap.org
    env:
      TILER_SERVER_PORT: 9090
      TILER_CACHE_TYPE: s3
      TILER_CACHE_BASEPATH: /mnt/data
      TILER_CACHE_MAX_ZOOM: 22
      # in case s3
      TILER_CACHE_REGION: us-east-1
      TILER_CACHE_BUCKET: tiler-cache-production
      TILER_CACHE_AWS_ACCESS_KEY_ID: {{PRODUCTION_TILER_CACHE_AWS_ACCESS_KEY_ID}}
      TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{PRODUCTION_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
      TILER_CACHE_AWS_ENDPOINT: ""
      EXECUTE_REINDEX: false
      EXECUTE_VACUUM_ANALYZE: false
    # In case you use TILER_CACHE_TYPE: file with  persistenceDisk 
    persistenceDisk:
      enabled: false
      accessMode: ReadWriteOnce
      mountPath: /mnt/data
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "2Gi"
        cpu: "1"
      limits:
        enabled: false
        memory: "4Gi"
        cpu: "2"
    autoscaling:
      enabled: true
      minReplicas: 4
      maxReplicas: 6
      cpuUtilization: 60 
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: tiler_db
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["tiler_db"] 
  # ====================================================================================================
  # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
  # ====================================================================================================
  tilerServerCacheCleaner:
    enabled: false
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    replicaCount: 1
    command: './cache_cleaner.sh'
    resources:
      enabled: true
      requests:
        enabled: false
        memory: "1Gi"
        cpu: "2"
      limits:
        enabled: false
        memory: "2Gi"
        cpu: "4"
    env:
      KILL_PROCESS: manually
      MAX_NUM_PS: 5
      PROCESS_NAME: tegola
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 1
      cpuUtilization: 90
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: tiler_db
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["tiler_db"] 
  # ====================================================================================================
  # Variables for Tasking Manager DB
  # ====================================================================================================
  tmDb:
    enabled: true
    priorityClass: medium-priority
    image:
      name: "postgis/postgis"
      tag: "14-3.3"
    env:
      POSTGRES_DB: tm
      POSTGRES_PASSWORD: {{PRODUCTION_TM_DB_PASSWORD}}
      POSTGRES_USER: postgres
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      subPath: postgresql-d
      AWS_ElasticBlockStore_volumeID: vol-05d28e1a06628bcb8
      AWS_ElasticBlockStore_size: 20Gi
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "1Gi"
        cpu: "2"
      limits:
        enabled: false
        memory: "2Gi"
        cpu: "2"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["database"] 
  # ====================================================================================================
  # Variables for Tasking Manager API
  # ====================================================================================================

  tmApi:
    enabled: true
    priorityClass: medium-priority
    replicaCount: 1
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '300'
    ingressDomain: tm-api.openhistoricalmap.org
    healthCheckPath: /api/docs
    env:
      TM_ORG_NAME: OpenHistoricalMap
      TM_ORG_CODE: OHM
      TM_ORG_URL: www.openhistoricalmap.org
      TM_ORG_PRIVACY_POLICY_URL: www.openhistoricalmap.org/copyright
      TM_ORG_GITHUB: github.com/openhistoricalmap
      OSM_SERVER_URL: https://www.openhistoricalmap.org
      OSM_NOMINATIM_SERVER_URL: https://nominatim.openhistoricalmap.org
      OSM_REGISTER_URL: https://www.openhistoricalmap.org/user/new
      ID_EDITOR_URL: https://www.openhistoricalmap.org/edit?editor=id
      POTLATCH2_EDITOR_URL: https://www.openhistoricalmap.org/edit?editor=potlatch2
      TM_SECRET: {{PRODUCTION_TM_API_SECRET}}
      TM_EMAIL_FROM_ADDRESS: web@noreply.openhistoricalmap.org
      TM_EMAIL_CONTACT_ADDRESS: admin@openhistoricalmap.org
      TM_SMTP_HOST: email-smtp.us-east-1.amazonaws.com
      TM_SMTP_PORT: 25
      TM_SMTP_USER: {{MAILER_USERNAME}}
      TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
      TM_DEFAULT_LOCALE: en
      TM_APP_API_URL: https://tm-api.openhistoricalmap.org
      TM_APP_BASE_URL: https://tasks.openhistoricalmap.org
      TM_IMPORT_MAX_FILESIZE: 3000000
      TM_MAX_AOI_AREA: 15000
      TM_APP_API_VERSION: v4
      # The following environment variables are for future versions of TM
      TM_CLIENT_ID: 2L3O78gSiOX6HHYa-Ktv7ckJqI-qGy3GxkgEf9b5FsM
      TM_CLIENT_SECRET: nQ-QkUiMftNm0yXRrKcZgrdjo4zkAfQWObTVxDECqNM
      TM_DEFAULT_CHANGESET_COMMENT: '#ohm-project'
      TM_REDIRECT_URI: https://tm-api.openhistoricalmap.org/authorized
      TM_SCOPE: 'read_prefs write_api'
      # Add extra info
      TM_ORG_FB: https://www.facebook.com/openhistoricalmap
      TM_ORG_INSTAGRAM: https://www.openhistoricalmap.org
      TM_ORG_TWITTER: https://x.com/OpenHistoricalMap
      TM_ORG_YOUTUBE: https://www.youtube.com/playlist?list=PLOi35w6_Hpx_CYdYBUpPeuiJ1djn5-wIx
    resources:
      enabled: false
      requests:
        enabled: false
        memory: 1Gi
        cpu: '2'
      limits:
        enabled: false
        memory: 2Gi
        cpu: '2'
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"] 
 # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
  nominatimUI:
    enabled: false
    ## Enable external service
    externalService:
      enabled: false
      ip: {{PRODUCTION_NOMINATIM_HOST}}
      port: 8084
    image:
      name: ghcr.io/openhistoricalmap/nominatim-ui
      tag: a469b5e
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
  nominatimApi:
    enabled: false
    ## Enable external service
    externalService:
      enabled: false
      ip: {{PRODUCTION_NOMINATIM_HOST}}
      port: 8083
    priorityClass: medium-priority
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: nominatim.openhistoricalmap.org
    replicaCount: 1
    env:
      PBF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-230727_1030.osm.pbf
      REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
      REPLICATION_UPDATE_INTERVAL: 60
      REPLICATION_RECHECK_INTERVAL: 30
      FREEZE: false
      IMPORT_WIKIPEDIA: false
      IMPORT_US_POSTCODES: false
      IMPORT_GB_POSTCODES: false
      IMPORT_TIGER_ADDRESSES: false
      THREADS: 8
      NOMINATIM_PASSWORD: {{PRODUCTION_NOMINATIM_PG_PASSWORD}}
      PGDATA: /var/lib/postgresql/16/main
      NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
      UPDATE_MODE: continuous
      OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
      IMPORT_STYLE: extratags
      EXTRA_TAGS: start_date,start_date:edtf,end_date,end_date:edt
    resources:
      enabled: false
      requests:
        enabled: false
        memory: '1Gi'
        cpu: '2'
      limits:
        enabled: false
        memory: '2Gi'
        cpu: '2'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/16/main
      subPath: nominatim-pgdata
      AWS_ElasticBlockStore_volumeID: vol-012355ce0247b5ccc
      AWS_ElasticBlockStore_size: 50Gi
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for overpass-api
# ====================================================================================================
  overpassApi:
    enabled: false
    ## Enable external service
    externalService:
      enabled: false
      ip: {{PRODUCTION_OVERPASS_HOST}}
      port: 8086
    priorityClass: medium-priority
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: overpass-api.openhistoricalmap.org
    env:
      OVERPASS_META: 'attic'
      OVERPASS_MODE: 'init'
      OVERPASS_PLANET_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240715_0003.osm.pbf
      OVERPASS_DIFF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute
      OVERPASS_RULES_LOAD: '10'
      OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
      OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1484000'
      OVERPASS_ALLOW_DUPLICATE_QUERIES: 'yes'
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      AWS_ElasticBlockStore_volumeID: vol-01d9060df5d14a903
      AWS_ElasticBlockStore_size: 100Gi
    resources:
      enabled: false
      requests:
        enabled: false
        memory: '1Gi'
        cpu: '2'
      limits:
        enabled: false
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for taginfo
# ====================================================================================================
  taginfo:
    enabled: true
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    serviceAnnotations:
      service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
    ingressDomain: taginfo.openhistoricalmap.org
    env:
      URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
      URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
      URL_PLANET_FILE: 'none'
      URL_HISTORY_PLANET_FILE: 'none'
      TIME_UPDATE_INTERVAL: 7d
      OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/main/images/taginfo/taginfo-config-production.json
      TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
      DOWNLOAD_DB: 'languages wiki'
      CREATE_DB: 'db projects chronology'
      ENVIRONMENT: production
      AWS_S3_BUCKET: taginfo
      INTERVAL_DOWNLOAD_DATA: 7d
      TAGINFO_DB_BASE_URL: https://planet.openhistoricalmap.org.s3.amazonaws.com/taginfo
    resources:
      enabled: false
    cronjob:
      enabled: true
      schedule: "0 2 */3 * *"
      nodeAffinity:
        enabled: true
        key: "nodegroup_type"
        values: ["job"]
      resources:
        enabled: true
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for osm-simple-metrics
# ====================================================================================================
  osmSimpleMetrics:
    enabled: true
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    schedule: '0 2 * * *'
    resources:
      enabled: false
      requests:
        enabled: false
        memory: '1Gi'
        cpu: '2'
      limits:
        enabled: false
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
  monitoringReplication:
    enabled: true
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    schedule: '*/30 * * * *'
    env:
      CREATE_MISSING_FILES: "empty"
      REPLICATION_SEQUENCE_NUMBER: "000000"
    resources:
      enabled: false
      requests:
        enabled: false
        memory: '1Gi'
        cpu: '2'
      limits:
        enabled: false
        memory: '2Gi'
        cpu: '2'
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
  # ====================================================================================================
  # Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
  # ====================================================================================================
  changesetReplicationJob:
    enabled: true
    priorityClass: medium-priority
    serviceAccount:
      enabled: true
      name: ohm-s3-bucket-access-production
    resources:
      enabled: false
      requests:
        enabled: false
        memory: '20Gi'
        cpu: '8'
      limits:
        enabled: false
        memory: '24Gi'
        cpu: '10'
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: true
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
  osmchaWeb:
    enabled: false
    priorityClass: medium-priority
    image:
      name: ghcr.io/openhistoricalmap/osmcha-frontend
      tag: df5e9b5
# ====================================================================================================
# Variables for osmcha Api
# ====================================================================================================
  osmchaApi:
    enabled: false
    priorityClass: medium-priority
    image:
      name: ghcr.io/openhistoricalmap/osmcha-django
      tag: 1bd58e1
    ingressDomain: osmcha.openhistoricalmap.org
    env:
      DJANGO_SETTINGS_MODULE: "config.settings.production"
      OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
      DJANGO_SECRET_KEY: {{PRODUCTION_OSMCHA_DJANGO_SECRET_KEY}}
      DJANGO_SECURE_SSL_REDIRECT: "False"
      OSM_SERVER_URL: https://www.openhistoricalmap.org
      OAUTH_REDIRECT_URI: https://osmcha.openhistoricalmap.org/authorized
      OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
      ## frontend
      OSMCHA_URL: https://osmcha.openhistoricalmap.org
      OSMCHA_API_URL: www.openhistoricalmap.org
      OAUTH2_OSM_KEY: 0KWrkIIQtFBc4k_Blb2WuIi108w10GYXFwW30qvk-W8
      OAUTH2_OSM_SECRET: UAnkzA3FqCP6ZYhqN1NEyQRdG36ALv46XGI7UtHJ0_o
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "512Mi"
        cpu: "1"
      limits:
        enabled: false
        memory: "512Mi"
        cpu: "1"
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
  osmchaDb:
    enabled: false
    priorityClass: medium-priority
    image:
      name: postgis/postgis
      tag: 17-3.5
    env:
      POSTGRES_DB: osmcha
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: {{PRODUCTION_OSMCHA_PG_PASSWORD}}
    resources:
      enabled: false
      requests:
        enabled: false
        memory: "20Gi"
        cpu: "8"
      limits:
        enabled: false
        memory: "24Gi"
        cpu: "10"
    persistenceDisk:
      enabled: true
      accessMode: ReadWriteOnce
      mountPath: /var/lib/postgresql/data
      AWS_ElasticBlockStore_volumeID: vol-033e4150d0378c45e
      AWS_ElasticBlockStore_size: 10Gi
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
# ====================================================================================================
# Variables for osmcha osm-adiff-service
# ====================================================================================================
  adiffService:
    enabled: false
    priorityClass: medium-priority
    image:
      name: ghcr.io/openhistoricalmap/osm-adiff-service
      tag: a343422497bb2ad4735ca7eada921b60eeb19e20
# ====================================================================================================
# Planet files server
# ====================================================================================================
  planetFiles:
    enabled: false
    priorityClass: low-priority

# ====================================================================================================
# Tiles cache SQS processor
# ====================================================================================================
ohm:
  tilerCache:
    enabled: false
    
  tilerCachePurge:
    enabled: false
    env:
      REGION_NAME: us-east-1
      NAMESPACE: default # Namespace to run the job
      SQS_QUEUE_URL: {{PRODUCTION_SQS_QUEUE_URL}}
      NODEGROUP_TYPE: web_large # Nodegroup type to run the purge and seed job
      # Maximum number of active jobs in high concurrency queue
      MAX_ACTIVE_JOBS: 10 
      DELETE_OLD_JOBS_AGE: 3600 # 1 hours
      ## Execute purging
      EXECUTE_PURGE: true
      PURGE_CONCURRENCY: 64
      PURGE_MIN_ZOOM: 3
      PURGE_MAX_ZOOM: 10 # Purging zoom 15,16,17,18,19,20 takes hours to complete,we are going to remove direct from s3 the tiles for zoom 19-20
      ## Execute seeding
      EXECUTE_SEED: false
      SEED_CONCURRENCY: 64
      SEED_MIN_ZOOM: 0
      SEED_MAX_ZOOM: 8
      ## Remove tiles from s3 for zoom levels
      ZOOM_LEVELS_TO_DELETE: 11,12,13,14,15,16,17,18,19,20
      S3_BUCKET_CACHE_TILER: tiler-cache-production
      S3_BUCKET_PATH_FILES: mnt/data/osm
    resources:
      enabled: false
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]

  tilerCacheSeedGlobal:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 128
      MIN_ZOOM: 0
      MAX_ZOOM: 7
    resources:
      enabled: false
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]

  tilerCacheSeedCoverage:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 128
      TILE_LIST_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/tile_coverage/tiles.list
      MIN_ZOOM: 8
      MAX_ZOOM: 14
    resources:
      enabled: false
    nodeSelector:
      enabled: false
      label_key: nodegroup_type
      label_value: web
    nodeAffinity:
      enabled: false
      key: "nodegroup_type"
      values: ["web"]
      