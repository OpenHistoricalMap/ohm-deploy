osm-seed:

    # ====================================================================================================
    # ====================================================================================================
    # ==================================Global Configurations=============================================
    # ====================================================================================================
    # ====================================================================================================
    
    # The version of the image group in osm-seed, get it here: https://hub.docker.com/r/developmentseed/osmseed-web/tags/
    # osmSeedVersion: ohm-b8a0ed1
  
    environment: staging
    # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
    cloudProvider: aws
  
    # ====================================================================================================
    # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
    # ====================================================================================================
    AWS_S3_BUCKET: {{STAGING_S3_BUCKET}}
  
    # ====================================================
    # AWS: Specify ARN for SSL certificate, currently assumes a single wildcard cert
    # ====================================================
  
    AWS_SSL_ARN: {{AWS_SSL_ARN}}
  
    # Specify serviceType.
    #
    # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
    # Use `NodePort` for local testing on minikube.
    #
    # The recommended setting is `ClusterIP`, and then following the instructions to
    # point a DNS record to the cluster IP address. This will setup the ingress rules
    # for all services as subdomains and configure SSL using Lets Encrypt.
    #
    # If you specify `LoadBalancer` as the service type, if you also specify
    # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
    # as the SSL certificate for your services. Else, you will need to configure
    # SSL separately. 
    serviceType: ClusterIP
    createClusterIssuer: true
    # Domain that is pointed to the clusterIP
    # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
    # Then, the cluster configuration will setup services at their respective subdomains:
    # - web.osmseed.example.com
    # - overpass.osmseed.example.com
    # - nominatim.osmseed.example.com
    # - etc.
    domain: staging.openhistoricalmap.org
  
    # ====================================================================================================
    # Configuration for Lets Encrypt setup
    # ====================================================================================================
  
    # Admin Email address used when generating Lets Encrypt certificates.
    # You will be notified of expirations, etc. on this email address.
    adminEmail: ohm-admins@googlegroups.com
    # ====================================================================================================
    # ====================================================================================================
    # ==================================Pods Configurations===============================================
    # ====================================================================================================
    # ====================================================================================================
    # ====================================================================================================
    # Variables for osm-seed database
    # ====================================================================================================
    db:
      enabled: true
      priorityClass: "high-priority"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
      env:
        POSTGRES_DB: {{STAGING_DB}}
        POSTGRES_USER: {{STAGING_DB_USER}}
        POSTGRES_PASSWORD: {{STAGING_DB_PASSWORD}}
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-db
        AWS_ElasticBlockStore_volumeID : vol-0d644eb9ea4e8fccf
        AWS_ElasticBlockStore_size: 200Gi
      resources:
        enabled: true
        requests:
          enabled: true
          memory: "1Gi"
          cpu: "200m"
        limits:
          enabled: false
          memory: "6Gi"
          cpu: "1700m"
      sharedMemorySize: 256Mi
      livenessProbeExec: true
      postgresqlConfig:
        enabled: true
        values: |
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 2GB
          work_mem = 20MB
          maintenance_work_mem = 512MB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 1GB
          min_wal_size = 80MB
          random_page_cost = 1.1
          effective_cache_size = 6GB
          log_min_duration_statement = 3000
          log_connections = on
          log_disconnections = on
          log_duration = off
          log_lock_waits = on
          log_statement = 'none'
          log_timezone = 'Etc/UTC'
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'
          # Parallelism settings
          max_parallel_workers_per_gather = 2
          max_parallel_workers = 4
          max_worker_processes = 4
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB
          session_preload_libraries = 'auto_explain'
          auto_explain.log_min_duration = '3s'
    # ====================================================================================================
    # Variables for osm-seed website
    # ====================================================================================================
    web:
      enabled: true
      priorityClass: "high-priority"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
      replicaCount: 1
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: www.staging.openhistoricalmap.org
      env:
        MAILER_ADDRESS: {{MAILER_ADDRESS}}
        MAILER_DOMAIN: staging.openhistoricalmap.org
        MAILER_USERNAME: {{MAILER_USERNAME}}
        MAILER_PASSWORD: {{MAILER_PASSWORD}}
        OSM_id_key: {{STAGING_ID_APPLICATION}}
        OAUTH_CLIENT_ID: {{STAGING_OAUTH_CLIENT_ID}}
        OAUTH_KEY: {{STAGING_OAUTH_KEY}}
        MAILER_FROM: ohm-admins@googlegroups.com
        NOMINATIM_URL: nominatim.staging.openhistoricalmap.org
        OVERPASS_URL: overpass-api.openhistoricalmap.org	
        NEW_RELIC_LICENSE_KEY: "none"
        NEW_RELIC_APP_NAME: "none"
        ORGANIZATION_NAME: OpenHistoricalMap
        WEBSITE_STATUS: "online"
        RAILS_CREDENTIALS_YML_ENC: {{STAGING_RAILS_CREDENTIALS_YML_ENC}}
        RAILS_MASTER_KEY: {{STAGING_RAILS_MASTER_KEY}}
        RAILS_ENV: production
        RAILS_LOG_LEVEL: warn
        RAILS_STORAGE_SERVICE: s3
        RAILS_STORAGE_REGION: us-east-1
        RAILS_STORAGE_BUCKET: ohm-website-staging
        EXTERNAL_CGIMAP: true
        PASSENGER_MAX_POOL_SIZE: 3
        OPENSTREETMAP_AUTH_ID: {{STAGING_OPENSTREETMAP_AUTH_ID}}
        OPENSTREETMAP_AUTH_SECRET: {{STAGING_OPENSTREETMAP_AUTH_SECRET}}
      resources:
        enabled: true
        requests:
          enabled: false
          memory: "0.5Gi"
          cpu: "200m"
        limits:
          enabled: false
          memory: "6Gi"
          cpu: "1700m"
      autoscaling:
        enabled: true
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization:
          enable: true
          value: 85
        memoryUtilization:
          enable: false
          value: 85
        behavior:
          scaleUp:
            stabilizationWindowSeconds: 60       # Wait 60 seconds of sustained high usage before scaling up.
            policyValue: 100                     # Allow up to 100% increase in replicas (e.g., 3 → 6).
            periodSeconds: 60                    # This increase limit applies within a 60-second window.
          scaleDown:
            stabilizationWindowSeconds: 120      # Wait 120 seconds (2 minutes) of low usage before scaling down.
            policyValue: 50                      # Allow up to 50% decrease in replicas at once (e.g., 6 → 3).
            periodSeconds: 60                    # This decrease limit applies within a 60-second window.
      sharedMemorySize: 16Mi
    # ====================================================================================================
    # Variables for memcached. Memcached is used to store session cookies
    # ====================================================================================================
    memcached:
      enabled: true
      priorityClass: "high-priority"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
      resources:
        enabled: false

    # ====================================================================================================
    # Cgimap
    # ====================================================================================================
    cgimap:
      enabled: true
      image:
        name: ghcr.io/openhistoricalmap/cgimap
        tag: 0.0.1-0.dev.git.2463.hb0e1e7f
      priorityClass: "high-priority"
      resources:
        enabled: true
        requests:
          enabled: true
          memory: "0.2Gi"
          cpu: "100m"
        limits:
          enabled: false
          memory: "4Gi"
          cpu: "1200m"
      nodeSelector:
        enabled: false
      autoscaling:
        enabled: true
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization: 90
        memoryUtilization: 90
    # ====================================================================================================
    # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
    # ====================================================================================================
    planetDump:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
      schedule: '* * * * *'
      env:
        OVERWRITE_PLANET_FILE: false
        PLANET_EXPORT_METHOD: planet-dump-ng
        DUMP_CLOUD_URL: s3://osmseed-staging/database/web-api-db/state.txt
        PLANET_EPOCH_DATE: '1970-01-01'
        PLANET_DUMP_NG_METADATA_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/refs/heads/staging/images/planet-dump/metadata.yml
      resources:
        enabled: false
  
    # ====================================================================================================
    # Variables for full-history container
    # ====================================================================================================
    fullHistory:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
      schedule: '0 4 * * *'
      env:
        OVERWRITE_FHISTORY_FILE: false
        PLANET_EXPORT_METHOD: planet-dump-ng
        DUMP_CLOUD_URL: s3://osmseed-staging/database/web-api-db/state.txt
        PLANET_EPOCH_DATE: '1970-01-01'
        PLANET_DUMP_NG_METADATA_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/refs/heads/staging/images/planet-dump/metadata.yml
      resources:
        enabled: false

    # ====================================================================================================
    # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
    # ====================================================================================================
    replicationJob:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
      env:
        ENABLE_SEND_SLACK_MESSAGE: "true"
        SLACK_WEBHOOK_URL: {{OHM_SLACK_WEBHOOK_URL}}
      resources:
        enabled: false
  
    # ====================================================================================================
    # Variables for osm-seed to pupulate the apidb
    # ====================================================================================================
    populateApidb:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
      resources:
        enabled: false

    # ====================================================================================================
    # Variables to start a pod to process osm files
    # ====================================================================================================
    osmProcessor:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env: 
        URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
        OSM_FILE_ACTION: simple_pbf
      resources:
        enabled: false
  
    # ====================================================================================================
    # Variables for restoring the DB
    # ====================================================================================================
    dbBackupRestore:
      cronjobs:
      - name: web-db
        enabled: false
        schedule: '0 0 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/web-api-db
          BACKUP_CLOUD_FILE: ohm-api-web-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
          RESTORE_URL_FILE: https://osmseed.s3.amazonaws.com/test.sql.gz
        resources:
          enabled: false
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job_xlarge
      - name: tm-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/tm-db
          BACKUP_CLOUD_FILE: ohm-tm-db
          AWS_S3_BUCKET: osmseed-staging
        resources:
          enabled: false
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: web_medium
      - name: osmcha-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: 'true'
          BACKUP_CLOUD_FOLDER: database/osmcha-db
          BACKUP_CLOUD_FILE: osmseed-osmcha-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
        resources:
          enabled: false
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job

    # ====================================================================================================
    # Variables for tiler-db
    # ====================================================================================================
  
    tilerDb:
      enabled: false
      priorityClass: medium-priority
      useExternalHost: # When we are using useExternalHost.enabled= true other variables are giong to be disable ans use the external host config
        enabled: true
      env:
        POSTGRES_HOST: {{STAGING_TILER_DB_HOST}}
        POSTGRES_DB: tiler_osm_production # Kuberntes existing db called,  tiler-osm
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_TILER_DB_PASSWORD}}
        POSTGRES_PORT: 5432
      sharedMemorySize: 128Mi
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        AWS_ElasticBlockStore_volumeID : vol-03fdf35caf55300d9
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      postgresqlConfig:
        enabled: false
        values: |
          # PostgreSQL configuration
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 4GB
          work_mem = 128MB
          maintenance_work_mem = 1GB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 2GB
          min_wal_size = 256MB
          random_page_cost = 1.1
          effective_cache_size = 12GB

          # Logging Settings
          log_min_duration_statement = 15000  # Log queries that take more than 15 seconds
          log_connections = on               # Log database connections
          log_disconnections = on            # Log database disconnections
          log_duration = off                 # Do not log the duration of all queries
          log_lock_waits = on                # Log when queries are waiting for locks
          log_statement = 'none'             # Avoid logging all queries by default
          log_timezone = 'Etc/UTC'

          # Locale Settings
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'

          # Parallelism Settings
          max_parallel_workers_per_gather = 4
          max_parallel_workers = 8
          max_worker_processes = 8
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB

          # Shared Libraries for Monitoring and Optimization
          shared_preload_libraries = 'auto_explain,pg_stat_statements'
          auto_explain.log_min_duration = '15s'  # Log queries explained by auto_explain that take more than 15 seconds

          # Timeout Settings
          tcp_keepalives_idle = 300
          tcp_keepalives_interval = 60
          tcp_keepalives_count = 10
          enable_mergejoin = false  # Disable merge joins for specific query patterns
          enable_hashjoin = false   # Disable hash joins for specific query patterns
          statement_timeout = '600s'  # Maximum time allowed for a query to complete (10 minutes)
          lock_timeout = '60s'        # Maximum time to wait for a lock (60 seconds)
          idle_in_transaction_session_timeout = '600s'  # Terminate idle transactions after 10 minutes

          # pg_stat_statements Settings
          pg_stat_statements.max = 10000  # Maximum number of queries tracked in pg_stat_statements
          pg_stat_statements.track = all  # Track all queries (DDL, DML, etc.)


    # ====================================================================================================
    # Variables for tiler-imposm
    # ====================================================================================================
  
    tilerImposm:
      enabled: false
      priorityClass: medium-priority
      env:
        TILER_IMPORT_FROM: osm
        TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-250207_0001.osm.pbf
        REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
        SEQUENCE_NUMBER: '1701000'
        OVERWRITE_STATE: false
        UPLOAD_EXPIRED_FILES: true
        IMPORT_NATURAL_EARTH: true
        IMPORT_OSM_LAND: true
        IMPOSM3_IMPORT_LAYERS: all
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        AWS_ElasticBlockStore_volumeID: vol-01ba324e4582c3206
        AWS_ElasticBlockStore_size: 50Gi
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
    # ====================================================================================================
    # Variables for tiler-server
    # ====================================================================================================
  
    tilerServer:
      enabled: true
      priorityClass: medium-priority
      externalService:
        enabled: true
        ip: {{STAGING_TILER_SERVER_HOST}}
        port: 9091
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      replicaCount: 1
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: vtiles.staging.openhistoricalmap.org
      env:
        TILER_SERVER_PORT: 9090
        TILER_CACHE_TYPE: s3
        TILER_CACHE_BASEPATH: /mnt/data
        TILER_CACHE_MAX_ZOOM: 22
        # in case s3
        TILER_CACHE_REGION: us-east-1
        TILER_CACHE_BUCKET: tiler-cache-staging
        TILER_CACHE_AWS_ACCESS_KEY_ID: {{STAGING_TILER_CACHE_AWS_ACCESS_KEY_ID}}
        TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{STAGING_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
        TILER_CACHE_AWS_ENDPOINT: ""
        # In case you use TILER_CACHE_TYPE: file with  persistenceDisk
        EXECUTE_REINDEX: false
        EXECUTE_VACUUM_ANALYZE: false
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        AWS_ElasticBlockStore_volumeID: vol-0e8b605fe3b16bf08
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization: 60
    # ====================================================================================================
    # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
    # ====================================================================================================
    tilerServerCacheCleaner:
      enabled: false
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      replicaCount: 1
      command: './cache_cleaner.sh' 
      resources:
        enabled: false
      env:
        KILL_PROCESS: manually
        MAX_NUM_PS: 4
        PROCESS_NAME: tegola
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 1
        cpuUtilization: 90
  
    # ====================================================================================================
    # Variables for Tasking Manager DB
    # ====================================================================================================
    tmDb:
      enabled: true
      priorityClass: medium-priority
      image:
        name: "postgis/postgis"
        tag: "14-3.3"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      env:
        POSTGRES_DB: tm
        POSTGRES_PASSWORD: {{STAGING_TM_DB_PASSWORD}}
        POSTGRES_USER: postgres
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        AWS_ElasticBlockStore_volumeID: vol-025e56094aa8e705b
        AWS_ElasticBlockStore_size: 20Gi
      resources:
        enabled: false
    # ====================================================================================================
    # Variables for Tasking Manager API
    # ====================================================================================================
  
    tmApi:
      enabled: true
      priorityClass: medium-priority
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      replicaCount: 1
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '300'
      ingressDomain: tm-api.staging.openhistoricalmap.org
      env:
        TM_ORG_NAME: OpenHistoricalMap
        TM_ORG_CODE: OHM
        TM_ORG_URL: staging.openhistoricalmap.org
        TM_ORG_PRIVACY_POLICY_URL: staging.openhistoricalmap.org/copyright
        TM_ORG_GITHUB: github.com/openhistoricalmap
        OSM_SERVER_URL: https://staging.openhistoricalmap.org
        OSM_NOMINATIM_SERVER_URL: https://nominatim.openhistoricalmap.org
        OSM_REGISTER_URL: https://staging.openhistoricalmap.org/user/new
        ID_EDITOR_URL: https://staging.openhistoricalmap.org/edit?editor=id
        POTLATCH2_EDITOR_URL: https://staging.openhistoricalmap.org/edit?editor=potlatch2
        TM_SECRET: {{STAGING_TM_API_SECRET}}
        TM_EMAIL_FROM_ADDRESS: ohm-admins@googlegroups.com
        TM_SMTP_HOST: email-smtp.us-east-1.amazonaws.com
        TM_SMTP_PORT: 25
        TM_SMTP_USER: {{MAILER_USERNAME}}
        TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
        TM_DEFAULT_LOCALE: en
        TM_APP_API_URL: https://tm-api.staging.openhistoricalmap.org
        TM_APP_BASE_URL: https://tasks-staging.openhistoricalmap.org
        TM_IMPORT_MAX_FILESIZE: 3000000
        TM_MAX_AOI_AREA: 15000
        TM_APP_API_VERSION: v2
        # The following environment variables are for future versions of TM
        TM_CLIENT_ID: EeFtCc-qwJEsKZWrD1jFQZfPHp5JpRq-da9jw55z86U
        TM_CLIENT_SECRET: g0TLdrT-IAu8VEhyuvJ_YBMWWUhSXXO75SpTxG2P3OI
        TM_DEFAULT_CHANGESET_COMMENT: 'Staging comments'
        TM_REDIRECT_URI: https://tasks-staging.openhistoricalmap.org/authorized
        TM_SCOPE: 'read_prefs write_api'
        # Add extra info
        TM_EMAIL_CONTACT_ADDRESS: ohm-admins@googlegroups.com
        TM_ORG_FB: https://www.facebook.com/openhistoricalmap
        TM_ORG_INSTAGRAM: https://www.openhistoricalmap.org
        TM_ORG_TWITTER: https://x.com/OpenHistoricalMap
        TM_ORG_YOUTUBE: https://www.youtube.com/playlist?list=PLOi35w6_Hpx_CYdYBUpPeuiJ1djn5-wIx

      resources:
        enabled: false
  
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
    nominatimUI:
      enabled: true
      image:
        name: ghcr.io/openhistoricalmap/nominatim-ui
        tag: a469b5e
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
    nominatimApi:
      enabled: false
      priorityClass: medium-priority
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '300'
      ingressDomain: nominatim.staging.openhistoricalmap.org
      replicaCount: 1
      env:
        PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-250213_0001.osm.pbf
        REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
        REPLICATION_UPDATE_INTERVAL: 60
        REPLICATION_RECHECK_INTERVAL: 30
        FREEZE: false
        IMPORT_WIKIPEDIA: false
        IMPORT_US_POSTCODES: false
        IMPORT_GB_POSTCODES: false
        IMPORT_TIGER_ADDRESSES: false
        THREADS: 8
        NOMINATIM_PASSWORD: {{STAGING_NOMINATIM_PG_PASSWORD}}
        PGDATA: /var/lib/postgresql/16/main
        NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
        UPDATE_MODE: continuous
        OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
        IMPORT_STYLE: extratags
        EXTRA_TAGS: start_date,start_date:edtf,end_date,end_date:edt
      resources:
        enabled: false
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/16/main
        subPath: nominatim-pgdata
        AWS_ElasticBlockStore_volumeID: vol-0874ffe0fc1703bba
        AWS_ElasticBlockStore_size: 50Gi
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
  # ====================================================================================================
  # Variables for overpass-api
  # ====================================================================================================
    overpassApi:
      enabled: false
      priorityClass: medium-priority
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: overpass-api.staging.openhistoricalmap.org
      env:
        OVERPASS_META: 'attic'
        OVERPASS_MODE: 'init'
        OVERPASS_PLANET_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240612_0002.osm.pbf
        OVERPASS_DIFF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute
        OVERPASS_RULES_LOAD: '10'
        OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
        OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1258000'
        OVERPASS_ALLOW_DUPLICATE_QUERIES: 'yes'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        AWS_ElasticBlockStore_volumeID: vol-000676289c153e301
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web_medium
  # ====================================================================================================
  # Variables for taginfo
  # ====================================================================================================
    taginfo:
      enabled: false
      priorityClass: medium-priority
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: taginfo.staging.openhistoricalmap.org
      env:
        URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
        URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
        URL_PLANET_FILE: 'none'
        URL_HISTORY_PLANET_FILE: 'none'
        TIME_UPDATE_INTERVAL: 7d
        OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/staging/images/taginfo/taginfo-config-staging.json
        TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
        DOWNLOAD_DB: 'languages wiki'
        CREATE_DB: 'db projects chronology'
        ENVIRONMENT: staging
        AWS_S3_BUCKET: taginfo
        INTERVAL_DOWNLOAD_DATA: 7d
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_medium
      cronjob:
        enabled: false
        schedule: "0 2 */3 * *"
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job_xlarge
        resources:
          enabled: true
          requests:
            memory: "13Gi"
            cpu: "3600m"
          limits:
            memory: "14Gi"
            cpu: "3800m"
  # ====================================================================================================
  # Variables for osm-simple-metrics
  # ====================================================================================================
    osmSimpleMetrics:
      enabled: false
      priorityClass: medium-priority
      schedule: '0 2 * * *'
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
    monitoringReplication:
      enabled: false
      priorityClass: medium-priority
      schedule: '*/30 * * * *'
      env:
        CREATE_MISSING_FILES: "empty"
        REPLICATION_SEQUENCE_NUMBER: "000000"
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
# ====================================================================================================
    changesetReplicationJob:
      enabled: false
      priorityClass: medium-priority
      resources:
        enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
    osmchaWeb:
      enabled: true
      priorityClass: medium-priority
      image:
        name: ghcr.io/openhistoricalmap/osmcha-frontend
        tag: f5cfe9c
# ====================================================================================================
# Variables for osmcha Api, We hard code the container id.
# ====================================================================================================
    osmchaApi:
      enabled: false
      priorityClass: medium-priority
      image:
        name: ghcr.io/openhistoricalmap/osmcha-django
        tag: 1bd58e1
      ingressDomain: osmcha.staging.openhistoricalmap.org
      env:
        DJANGO_SETTINGS_MODULE: "config.settings.production"
        OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
        DJANGO_SECRET_KEY: {{STAGING_OSMCHA_DJANGO_SECRET_KEY}}
        DJANGO_SECURE_SSL_REDIRECT: "False"
        OSM_SERVER_URL: https://www.openhistoricalmap.org
        OAUTH_REDIRECT_URI: https://osmcha.staging.openhistoricalmap.org/authorized
        OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
        ## frontend
        OSMCHA_URL: https://osmcha.staging.openhistoricalmap.org
        OSMCHA_API_URL: www.openhistoricalmap.org
        OAUTH2_OSM_KEY: tHRgc_GVLsUj2tuVQSg05nn5f8Tnda8mF-ZfEbI3ItA
        OAUTH2_OSM_SECRET: s9blyq71UditeidYeiQc_J40-DffUh3EVlDCGVcskQI
      resources:
        enabled: false
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      fetch_changesets_cronjob: '*/2 * * * *'
      process_changesets_cronjob: '0 * * * *'
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
    osmchaDb:
      enabled: false
      priorityClass: medium-priority
      image:
        name: postgis/postgis
        tag: 17-3.5
      env:
        POSTGRES_DB: osmcha
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_OSMCHA_PG_PASSWORD}}
      resources:
        enabled: false
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        AWS_ElasticBlockStore_volumeID: vol-01924d8040c54d067
        AWS_ElasticBlockStore_size: 10Gi
      nodeSelector:
        enabled: false
# ====================================================================================================
# Variables for osmcha osm-adiff-service
# ====================================================================================================
    adiffService:
      enabled: false
      priorityClass: medium-priority
      image:
        name: ghcr.io/openhistoricalmap/osm-adiff-service
        tag: a343422497bb2ad4735ca7eada921b60eeb19e20
# ====================================================================================================
# Planet files server
# ====================================================================================================
    planetFiles:
      enabled: false

# ====================================================================================================
# Tiles cache SQS processor
# ====================================================================================================
ohm:
  tilerCache:
    enabled: false
    
  tilerCachePurge:
    enabled: false
    env:
      REGION_NAME: us-east-1
      NAMESPACE: default # Namespace to run the job
      SQS_QUEUE_URL: {{STAGING_SQS_QUEUE_URL}}
      NODEGROUP_TYPE: web_medium # Nodegroup type to run the purge and seed job
      # Maximum number of active jobs in high concurrency queue
      MAX_ACTIVE_JOBS: 2 
      DELETE_OLD_JOBS_AGE: 3600 # 1 hours
      ## Execute purging
      EXECUTE_PURGE: true
      PURGE_CONCURRENCY: 64
      PURGE_MIN_ZOOM: 3
      PURGE_MAX_ZOOM: 12 # Purging zoom 15,16,17,18,19,20 takes hours to complete,we are going to remove direct from s3 the tiles for zoom 19-20
      ## Execute seeding
      EXECUTE_SEED: false
      SEED_CONCURRENCY: 64
      SEED_MIN_ZOOM: 0
      SEED_MAX_ZOOM: 12
      ## Remove tiles from s3 for zoom levels
      ZOOM_LEVELS_TO_DELETE: 13,14,15,16,17,18,19,20
      S3_BUCKET_CACHE_TILER: tiler-cache-staging
      S3_BUCKET_PATH_FILES: mnt/data/osm
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_medium

  tilerCacheSeedGlobal:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 32
      MIN_ZOOM: 0
      MAX_ZOOM: 7
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_medium

  tilerCacheSeedCoverage:
    enabled: false
    schedule: '0 * * * *'
    env:
      CONCURRENCY: 32
      TILE_LIST_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/tile_coverage/tiles.list
      MIN_ZOOM: 8
      MAX_ZOOM: 14
    resources:
      enabled: false
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_medium
