osm-seed:

    # ====================================================================================================
    # ====================================================================================================
    # ==================================Global Configurations=============================================
    # ====================================================================================================
    # ====================================================================================================
    
    # The version of the image group in osm-seed, get it here: https://hub.docker.com/r/developmentseed/osmseed-web/tags/
    # osmSeedVersion: ohm-b8a0ed1
  
    environment: staging
    # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
    cloudProvider: aws
  
    # ====================================================================================================
    # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
    # ====================================================================================================
    AWS_S3_BUCKET: {{STAGING_S3_BUCKET}}
  
    # ====================================================
    # AWS: Specify ARN for SSL certificate, currently assumes a single wildcard cert
    # ====================================================
  
    AWS_SSL_ARN: {{AWS_SSL_ARN}}
  
    # Specify serviceType.
    #
    # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
    # Use `NodePort` for local testing on minikube.
    #
    # The recommended setting is `ClusterIP`, and then following the instructions to
    # point a DNS record to the cluster IP address. This will setup the ingress rules
    # for all services as subdomains and configure SSL using Lets Encrypt.
    #
    # If you specify `LoadBalancer` as the service type, if you also specify
    # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
    # as the SSL certificate for your services. Else, you will need to configure
    # SSL separately. 
    serviceType: ClusterIP
  
    # Domain that is pointed to the clusterIP
    # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
    # Then, the cluster configuration will setup services at their respective subdomains:
    # - web.osmseed.example.com
    # - overpass.osmseed.example.com
    # - nominatim.osmseed.example.com
    # - etc.
    domain: staging.openhistoricalmap.org
  
    # ====================================================================================================
    # Configuration for Lets Encrypt setup
    # ====================================================================================================
  
    # Admin Email address used when generating Lets Encrypt certificates.
    # You will be notified of expirations, etc. on this email address.
    adminEmail: no-reply@staging.openhistoricalmap.org
    # ====================================================================================================
    # ====================================================================================================
    # ==================================Pods Configurations===============================================
    # ====================================================================================================
    # ====================================================================================================
    # ====================================================================================================
    # Variables for osm-seed database
    # ====================================================================================================
    db:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db
      env:
        POSTGRES_DB: {{STAGING_DB}}
        POSTGRES_USER: {{STAGING_DB_USER}}
        POSTGRES_PASSWORD: {{STAGING_DB_PASSWORD}}
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-db
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : {{STAGING_DB_EBS}}
        AWS_ElasticBlockStore_size: 200Gi
      resources:
        enabled: false
        requests:
          memory: "10Gi"
          cpu: "5"
        limits:
          memory: "10Gi"
          cpu: "5"
      sharedMemorySize: 256Mi
      livenessProbeExec: true
    # ====================================================================================================
    # Variables for osm-seed website
    # ====================================================================================================
    web:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: www.staging.openhistoricalmap.org
      env:
        MAILER_ADDRESS: {{MAILER_ADDRESS}}
        MAILER_DOMAIN: staging.openhistoricalmap.org
        MAILER_USERNAME: {{MAILER_USERNAME}}
        MAILER_PASSWORD: {{MAILER_PASSWORD}}
        OSM_id_key: {{STAGING_ID_APPLICATION}}
        OAUTH_CLIENT_ID: {{STAGING_OAUTH_CLIENT_ID}}
        OAUTH_KEY: {{STAGING_OAUTH_KEY}}
        MAILER_FROM: no-reply@staging.openhistoricalmap.org
        NOMINATIM_URL: nominatim-api.staging.openhistoricalmap.org
        OVERPASS_URL: overpass-api.staging.openhistoricalmap.org	
        NEW_RELIC_LICENSE_KEY: {{STAGING_NEW_RELIC_LICENSE_KEY}}
        NEW_RELIC_APP_NAME: {{STAGING_NEW_RELIC_APP_NAME}}
        ORGANIZATION_NAME: OpenHistoricalMap
      resources:
        enabled: false
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "2Gi"
          cpu: "2"
      autoscaling:
        enabled: false
        minReplicas: 2
        maxReplicas: 10
        cpuUtilization: 80
      sharedMemorySize: 16Mi
    # ====================================================================================================
    # Variables for memcached. Memcached is used to store session cookies
    # ====================================================================================================
    memcached:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      resources:
        enabled: false
        requests:
          memory: "8Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "2"
    # ====================================================================================================
    # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
    # ====================================================================================================
    planetDump:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
      schedule: '0 0 * * *'
      env:
        OVERWRITE_PLANET_FILE: true
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for full-history container
    # ====================================================================================================
    fullHistory:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
      schedule: '0 0 * * *'
      env:
        OVERWRITE_FHISTORY_FILE: false
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for id-editor
    # ====================================================================================================
    idEditor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: 35.233.232.82
      env:
        ID_EDITOR_PORT: 80
        OSM_API_PROTOCOL: http
        OSM_API_DOMAIN: a03968e659140407d87c7e30e02372af-131263895.us-east-1.elb.amazonaws.com
        OAUTH_CONSUMER_KEY: 8NxmyUVwO4beulAzSoMv1zOcghz2rWrwmrLZ4Sis
        OAUTH_SECRET: q30neDRV1oqDArJyqMEnGVDr0BnUNVUiwxM7QuHN
      resources:
        enabled: false
        requests:
          memory: "300Mi"
          cpu: "0.4"
        limits:
          memory: "400Mi"
          cpu: "0.5"
  
    # ====================================================================================================
    # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
    # ====================================================================================================
    replicationJob:
      enabled: true
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
  
    # ====================================================================================================
    # Variables for osm-seed to pupulate the apidb
    # ====================================================================================================
    populateApidb:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2.5"
  
    # ====================================================================================================
    # Variables to start a pod to process osm files
    # ====================================================================================================
    osmProcessor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env: 
        URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
        OSM_FILE_ACTION: simple_pbf
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for restoring the DB
    # ====================================================================================================
    dbBackupRestore:
      cronjobs:
      - name: web-db
        enabled: false
        schedule: '0 0 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/web-api-db
          BACKUP_CLOUD_FILE: ohm-api-web-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
          RESTORE_URL_FILE: https://osmseed.s3.amazonaws.com/test.sql.gz
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job
      - name: osmcha-db
        enabled: false
        schedule: '0 0 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: 'true'
          BACKUP_CLOUD_FOLDER: database/osmcha-db
          BACKUP_CLOUD_FILE: osmseed-osmcha-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job

    # ====================================================================================================
    # Variables for tiler-db
    # ====================================================================================================
  
    tilerDb:
      enabled: true
      env:
        POSTGRES_HOST: staging-tiler-db
        POSTGRES_DB: tiler-osm
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_TILER_DB_PASSWORD}}
        POSTGRES_PORT: 5432
        # for 20Gi
        # POSTGRES_DB_MAX_CONNECTIONS: 500
        # POSTGRES_DB_SHARED_BUFFERS: 5GB
        # POSTGRES_DB_WORK_MEM: 15MB
        # POSTGRES_DB_MAINTENANCE_WORK_MEM: 1GB
        # POSTGRES_DB_EFFECTIVE_CACHE_SIZE: 10GB
        #for 15Gi
        POSTGRES_DB_MAX_CONNECTIONS: 500
        POSTGRES_DB_SHARED_BUFFERS: 3GB
        POSTGRES_DB_WORK_MEM: 7MB
        POSTGRES_DB_MAINTENANCE_WORK_MEM: 750MB
        POSTGRES_DB_EFFECTIVE_CACHE_SIZE: 7GB
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : vol-0de0a9672fa9fa7e4
        AWS_ElasticBlockStore_size: 40Gi
      resources:
        enabled: false
        requests:
          memory: '8Gi'
          cpu: '4'
        limits:
          memory: '8Gi'
          cpu: '2'
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: db
    # ====================================================================================================
    # Variables for tiler-imposm
    # ====================================================================================================
  
    tilerImposm:
      enabled: true
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      env:
        TILER_IMPORT_FROM: osm
        TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240404_0003.osm.pbf
        REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
        SEQUENCE_NUMBER: "1351000"
        OVERWRITE_STATE: false
        UPLOAD_EXPIRED_FILES: true
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-0af76080fb2dfc3c0
        AWS_ElasticBlockStore_size: 40Gi
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
    # ====================================================================================================
    # Variables for tiler-server
    # ====================================================================================================
  
    tilerServer:
      enabled: true
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      commad: './start.sh'
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: vtiles.staging.openhistoricalmap.org
      env:
        TILER_SERVER_PORT: 9090
        TILER_CACHE_TYPE: s3
        TILER_CACHE_BASEPATH: /mnt/data
        TILER_CACHE_MAX_ZOOM: 22
        # in case s3
        TILER_CACHE_REGION: us-east-1
        TILER_CACHE_BUCKET: tiler-cache-staging
        TILER_CACHE_AWS_ACCESS_KEY_ID: {{STAGING_TILER_CACHE_AWS_ACCESS_KEY_ID}}
        TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{STAGING_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
        # In case you use TILER_CACHE_TYPE: file with  persistenceDisk
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-0e8b605fe3b16bf08
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "10Gi"
          cpu: "2"
      autoscaling:
        enabled: true
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization: 60
    # ====================================================================================================
    # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
    # ====================================================================================================
    tilerServerCacheCleaner:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      command: './cache_cleaner.sh'
      resources:
        enabled: false
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "10Gi"
          cpu: "2"
      env:
        KILL_PROCESS: manually
        MAX_NUM_PS: 4
        PROCESS_NAME: tegola
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 1
        cpuUtilization: 90
    # ====================================================================================================
    # Variables for tiler-visor
    # ====================================================================================================
    tilerVisor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: tiler
      replicaCount: 1
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: 35.233.232.82
      env:
        TILER_VISOR_PROTOCOL: http
        TILER_VISOR_PORT: 8081
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
  
  
    # ====================================================================================================
    # Variables for Tasking Manager API
    # ====================================================================================================
  
    tmApi:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: tm-api.staging.openhistoricalmap.org
      env:
        POSTGRES_HOST: {{STAGING_TM_API_DB_HOST}}
        POSTGRES_DB: {{STAGING_TM_API_DB}}
        POSTGRES_PASSWORD: {{STAGING_TM_API_DB_PASSWORD}}
        POSTGRES_USER: {{STAGING_TM_API_DB_USER}}
        POSTGRES_PORT: 5432
        TM_ORG_NAME: 'OpenHistoricalMap'
        TM_ORG_CODE: 'OHM'
        TM_ORG_URL: 'openhistoricalmap.org'
        TM_ORG_PRIVACY_POLICY_URL: 'staging.openhistoricalmap.org/copyright'
        TM_ORG_GITHUB: 'github.com/openhistoricalmap'
        OSM_SERVER_URL: 'https://staging.openhistoricalmap.org'
        OSM_NOMINATIM_SERVER_URL: 'https://nominatim-api.staging.openhistoricalmap.org'
        OSM_REGISTER_URL: 'https://staging.openhistoricalmap.org/user/new'
        ID_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=id'
        POTLATCH2_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=potlatch2'
        TM_SECRET: {{STAGING_TM_API_SECRET}}
        TM_CONSUMER_KEY: {{STAGING_TM_API_CONSUMER_KEY}}
        TM_CONSUMER_SECRET: {{STAGING_TM_API_CONSUMER_SECRET}}
        TM_EMAIL_FROM_ADDRESS: 'no-reply@openhistoricalmap.org'
        TM_SMTP_HOST: 'email-smtp.us-east-1.amazonaws.com'
        TM_SMTP_PORT: 25
        TM_SMTP_USER: {{MAILER_USERNAME}}
        TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
        TM_DEFAULT_LOCALE: 'en'
        TM_APP_API_URL: 'https://tm-api.staging.openhistoricalmap.org'
        TM_APP_BASE_URL: 'https://tasks.staging.openhistoricalmap.org'
        TM_IMPORT_MAX_FILESIZE: 3000000
        TM_MAX_AOI_AREA: 15000
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
  
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
    nominatimApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: nominatim-api.staging.openhistoricalmap.org
      replicaCount: 1
      env:
        PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240405_0003.osm.pbf
        REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
        REPLICATION_UPDATE_INTERVAL: 60
        REPLICATION_RECHECK_INTERVAL: 30
        FREEZE: false
        IMPORT_WIKIPEDIA: false
        IMPORT_US_POSTCODES: false
        IMPORT_GB_POSTCODES: false
        IMPORT_TIGER_ADDRESSES: false
        THREADS: 8
        NOMINATIM_PASSWORD: {{STAGING_NOMINATIM_PG_PASSWORD}}
        PGDATA: /var/lib/postgresql/14/main
        NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
        UPDATE_MODE: continuous
        OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/14/main
        subPath: nominatim-pgdata
        # Minikube
        localVolumeHostPath: /mnt/nominatim-db-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-0a9f1da7c7f6d587e
        AWS_ElasticBlockStore_size: 30Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-disk-nominatim_db-v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_xlarge
  # ====================================================================================================
  # Variables for overpass-api
  # ====================================================================================================
    overpassApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: overpass-api.staging.openhistoricalmap.org
      env:
        OVERPASS_META: 'attic'
        OVERPASS_MODE: 'init'
        OVERPASS_PLANET_URL: 'https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240102_0000.osm.pbf'
        OVERPASS_DIFF_URL: 'http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute'
        OVERPASS_RULES_LOAD: '10'
        OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
        OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1258000'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        AWS_ElasticBlockStore_volumeID: vol-0da7d726731c27708
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: '8Gi'
          cpu: '2'
        limits:
          memory: '8Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
  # ====================================================================================================
  # Variables for taginfo
  # ====================================================================================================
    taginfo:
      enabled: true
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: taginfo.staging.openhistoricalmap.org
      env:
        URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
        URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
        URL_PLANET_FILE: 'none'
        URL_HISTORY_PLANET_FILE: 'none'
        TIME_UPDATE_INTERVAL: 7d
        OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/staging/images/taginfo/taginfo-config-staging.json
        TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
        DOWNLOAD_DB: 'languages wiki'
        CREATE_DB: 'db projects chronology'
        ENVIRONMENT: staging
        AWS_S3_BUCKET: taginfo
        INTERVAL_DOWNLOAD_DATA: 3600
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      cronjob:
        enabled: false
        schedule: "* * * * *"
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job_xlarge
  # ====================================================================================================
  # Variables for osm-simple-metrics
  # ====================================================================================================
    osmSimpleMetrics:
      enabled: true
      schedule: '0 2 * * *'
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
    monitoringReplication:
      enabled: true
      schedule: '*/30 * * * *'
      env:
        CREATE_MISSING_FILES: "empty"
        REPLICATION_SEQUENCE_NUMBER: "000000"
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
# ====================================================================================================
    changesetReplicationJob:
      enabled: true
      resources:
        enabled: false
        requests:
          memory: '20Gi'
          cpu: '8'
        limits:
          memory: '24Gi'
          cpu: '10'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
    osmchaWeb:
      enabled: true
# ====================================================================================================
# Variables for osmcha Api, We hard code the container id.
# ====================================================================================================
    osmchaApi:
      enabled: true
      image:
        name: "ghcr.io/openhistoricalmap/osmcha-django"
        tag: "a1bcea85dc1f7c27566c20bafe7fff7aaa1e38a4"
      ingressDomain: osmcha.staging.openhistoricalmap.org
      env:
        DJANGO_SETTINGS_MODULE: "config.settings.production"
        OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
        DJANGO_SECRET_KEY: {{STAGING_OSMCHA_DJANGO_SECRET_KEY}}        
        OAUTH_OSM_KEY: {{STAGING_OSMCHA_API_CONSUMER_KEY}}
        OAUTH_OSM_SECRET: {{STAGING_OSMCHA_API_CONSUMER_SECRET}}
        DJANGO_SECURE_SSL_REDIRECT: "False"
        OSM_SERVER_URL: https://www.openhistoricalmap.org
        OAUTH_REDIRECT_URI: https://osmcha.staging.openhistoricalmap.org/oauth-landing.html
        OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
        ## frontend
        OSMCHA_URL: https://osmcha.staging.openhistoricalmap.org
        OSMCHA_API_URL: www.openhistoricalmap.org
        REACT_APP_OSM_URL: https://www.openhistoricalmap.org
        REACT_APP_OSM_API: https://www.openhistoricalmap.org/api/0.6
        REACT_APP_OVERPASS_BASE: //overpass-api.openhistoricalmap.org/api/interpreter
        REACT_APP_ENABLE_REAL_CHANGESETS: 0
        REACT_APP_MAPBOX_ACCESS_TOKEN: {{STAGING_OSMCHA_REACT_APP_MAPBOX_ACCESS_TOKEN}}
      resources:
        enabled: false
        requests:
          memory: "512Mi"
          cpu: "1"
        limits:
          memory: "512Mi"
          cpu: "1"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
    osmchaDb:
      enabled: true
      image:
        name: "developmentseed/osmseed-osmcha-db"
        tag: "0.1.0-n767.h0090e97"
      env:
        POSTGRES_DB: {{STAGING_OSMCHA_PG_DATABASE}}
        POSTGRES_USER: {{STAGING_OSMCHA_PG_USER}}
        POSTGRES_PASSWORD: {{STAGING_OSMCHA_PG_PASSWORD}}
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        # Minikube
        localVolumeHostPath: /mnt/db-data/osmcha-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-06f981afe8def5915
        AWS_ElasticBlockStore_size: 10Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-osmcha-disk--v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: false
# ====================================================================================================
# Planet files server
# ====================================================================================================
    planetFiles:
      enabled: false
      