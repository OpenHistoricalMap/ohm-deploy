osm-seed:

    # ====================================================================================================
    # ====================================================================================================
    # ==================================Global Configurations=============================================
    # ====================================================================================================
    # ====================================================================================================
    
    # The version of the image group in osm-seed, get it here: https://hub.docker.com/r/developmentseed/osmseed-web/tags/
    # osmSeedVersion: ohm-b8a0ed1
  
    environment: staging
    # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
    cloudProvider: aws
  
    # ====================================================================================================
    # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
    # ====================================================================================================
    AWS_S3_BUCKET: {{STAGING_S3_BUCKET}}
  
    # ====================================================
    # AWS: Specify ARN for SSL certificate, currently assumes a single wildcard cert
    # ====================================================
  
    AWS_SSL_ARN: {{AWS_SSL_ARN}}
  
    # Specify serviceType.
    #
    # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
    # Use `NodePort` for local testing on minikube.
    #
    # The recommended setting is `ClusterIP`, and then following the instructions to
    # point a DNS record to the cluster IP address. This will setup the ingress rules
    # for all services as subdomains and configure SSL using Lets Encrypt.
    #
    # If you specify `LoadBalancer` as the service type, if you also specify
    # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
    # as the SSL certificate for your services. Else, you will need to configure
    # SSL separately. 
    serviceType: ClusterIP
    createClusterIssuer: true
    # Domain that is pointed to the clusterIP
    # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
    # Then, the cluster configuration will setup services at their respective subdomains:
    # - web.osmseed.example.com
    # - overpass.osmseed.example.com
    # - nominatim.osmseed.example.com
    # - etc.
    domain: staging.openhistoricalmap.org
  
    # ====================================================================================================
    # Configuration for Lets Encrypt setup
    # ====================================================================================================
  
    # Admin Email address used when generating Lets Encrypt certificates.
    # You will be notified of expirations, etc. on this email address.
    adminEmail: ohm-admins@googlegroups.com
    # ====================================================================================================
    # ====================================================================================================
    # ==================================Pods Configurations===============================================
    # ====================================================================================================
    # ====================================================================================================
    # ====================================================================================================
    # Variables for osm-seed database
    # ====================================================================================================
    db:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db
      env:
        POSTGRES_DB: {{STAGING_DB}}
        POSTGRES_USER: {{STAGING_DB_USER}}
        POSTGRES_PASSWORD: {{STAGING_DB_PASSWORD}}
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-db
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : vol-06464c17e2c8f9895
        AWS_ElasticBlockStore_size: 200Gi
      resources:
        enabled: false
        requests:
          memory: "6Gi"
          cpu: "1500m"
        limits:
          memory: "6Gi"
          cpu: "1700m"
      sharedMemorySize: 256Mi
      livenessProbeExec: true
      postgresqlConfig:
        enabled: true
        values: |
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 2GB
          work_mem = 20MB
          maintenance_work_mem = 512MB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 1GB
          min_wal_size = 80MB
          random_page_cost = 1.1
          effective_cache_size = 6GB
          log_min_duration_statement = 3000
          log_connections = on
          log_disconnections = on
          log_duration = off
          log_lock_waits = on
          log_statement = 'none'
          log_timezone = 'Etc/UTC'
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'
          # Parallelism settings
          max_parallel_workers_per_gather = 2
          max_parallel_workers = 4
          max_worker_processes = 4
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB
          session_preload_libraries = 'auto_explain'
          auto_explain.log_min_duration = '3s'
    # ====================================================================================================
    # Variables for osm-seed website
    # ====================================================================================================
    web:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: www.staging.openhistoricalmap.org
      env:
        MAILER_ADDRESS: {{MAILER_ADDRESS}}
        MAILER_DOMAIN: staging.openhistoricalmap.org
        MAILER_USERNAME: {{MAILER_USERNAME}}
        MAILER_PASSWORD: {{MAILER_PASSWORD}}
        OSM_id_key: {{STAGING_ID_APPLICATION}}
        OAUTH_CLIENT_ID: {{STAGING_OAUTH_CLIENT_ID}}
        OAUTH_KEY: {{STAGING_OAUTH_KEY}}
        MAILER_FROM: ohm-admins@googlegroups.com
        NOMINATIM_URL: nominatim-api.staging.openhistoricalmap.org
        OVERPASS_URL: overpass-api.staging.openhistoricalmap.org	
        NEW_RELIC_LICENSE_KEY: {{STAGING_NEW_RELIC_LICENSE_KEY}}
        NEW_RELIC_APP_NAME: {{STAGING_NEW_RELIC_APP_NAME}}
        ORGANIZATION_NAME: OpenHistoricalMap
        WEBSITE_STATUS: "online"
        # API_TIMEOUT: 600
        # WEB_TIMEOUT: 600
      resources:
        enabled: true
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "2Gi"
          cpu: "1"
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 10
        cpuUtilization: 80
      sharedMemorySize: 16Mi
    # ====================================================================================================
    # Variables for memcached. Memcached is used to store session cookies
    # ====================================================================================================
    memcached:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      resources:
        enabled: false
        requests:
          memory: "8Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "2"
    # ====================================================================================================
    # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
    # ====================================================================================================
    planetDump:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
      schedule: '0 0 * * *'
      env:
        OVERWRITE_PLANET_FILE: true
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for full-history container
    # ====================================================================================================
    fullHistory:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job
      schedule: '0 0 * * *'
      env:
        OVERWRITE_FHISTORY_FILE: false
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for id-editor
    # ====================================================================================================
    idEditor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: 35.233.232.82
      env:
        ID_EDITOR_PORT: 80
        OSM_API_PROTOCOL: http
        OSM_API_DOMAIN: a03968e659140407d87c7e30e02372af-131263895.us-east-1.elb.amazonaws.com
        OAUTH_CONSUMER_KEY: 8NxmyUVwO4beulAzSoMv1zOcghz2rWrwmrLZ4Sis
        OAUTH_SECRET: q30neDRV1oqDArJyqMEnGVDr0BnUNVUiwxM7QuHN
      resources:
        enabled: false
        requests:
          memory: "300Mi"
          cpu: "0.4"
        limits:
          memory: "400Mi"
          cpu: "0.5"
  
    # ====================================================================================================
    # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
    # ====================================================================================================
    replicationJob:
      enabled: true
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
  
    # ====================================================================================================
    # Variables for osm-seed to pupulate the apidb
    # ====================================================================================================
    populateApidb:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2.5"
  
    # ====================================================================================================
    # Variables to start a pod to process osm files
    # ====================================================================================================
    osmProcessor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env: 
        URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
        OSM_FILE_ACTION: simple_pbf
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for restoring the DB
    # ====================================================================================================
    dbBackupRestore:
      cronjobs:
      - name: web-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/web-api-db
          BACKUP_CLOUD_FILE: ohm-api-web-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
          RESTORE_URL_FILE: https://osmseed.s3.amazonaws.com/test.sql.gz
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job
      - name: tm-db
        enabled: true
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/tm-db
          BACKUP_CLOUD_FILE: ohm-tm-db
          AWS_S3_BUCKET: osmseed-staging
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job
      - name: osmcha-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: 'true'
          BACKUP_CLOUD_FOLDER: database/osmcha-db
          BACKUP_CLOUD_FILE: osmseed-osmcha-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job

    # ====================================================================================================
    # Variables for tiler-db
    # ====================================================================================================
  
    tilerDb:
      enabled: true
      env:
        POSTGRES_HOST: staging-tiler-db
        POSTGRES_DB: tiler-osm
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_TILER_DB_PASSWORD}}
        POSTGRES_PORT: 5432
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : vol-0df81bcd6d6a70603
        AWS_ElasticBlockStore_size: 100Gi
      resources:
          enabled: true
          requests:
            memory: "6Gi"
            cpu: "1500m"
          limits:
            memory: "6Gi"
            cpu: "1700m"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db
      postgresqlConfig:
        enabled: true
        values: |
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 4GB
          work_mem = 128MB
          maintenance_work_mem = 1GB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 2GB
          min_wal_size = 256MB
          random_page_cost = 1.1
          effective_cache_size = 8GB
          log_min_duration_statement = 15000
          log_connections = on
          log_disconnections = on
          log_duration = off
          log_lock_waits = on
          log_statement = 'none'
          log_timezone = 'Etc/UTC'
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'

          # Parallelism settings
          max_parallel_workers_per_gather = 4
          max_parallel_workers = 8
          max_worker_processes = 8
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB

          session_preload_libraries = 'auto_explain'
          auto_explain.log_min_duration = '15s'

          # Timeout settings
          tcp_keepalives_idle = 300
          tcp_keepalives_interval = 60
          tcp_keepalives_count = 10

          # Disable join options for routes
          enable_mergejoin = false
          enable_hashjoin = false

          # Complete large queries statements
          statement_timeout = '60s'
          lock_timeout = '5s'
          idle_in_transaction_session_timeout = '10s'

# Incrementar el tamaño del buffer compartido
shared_buffers = 4GB

# Aumentar el trabajo de memoria
work_mem = 128MB

# Aumentar la memoria de mantenimiento
maintenance_work_mem = 1GB

# Incrementar el tamaño efectivo del caché
effective_cache_size = 12GB

# Ajustar los tiempos de espera
statement_timeout = '600s'    # 10 minutos
lock_timeout = '60s'
idle_in_transaction_session_timeout = '600s'  # 10 minutos

    # ====================================================================================================
    # Variables for tiler-imposm
    # ====================================================================================================
  
    tilerImposm:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        TILER_IMPORT_FROM: osm
        TILER_IMPORT_PBF_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240726_0003.osm.pbf
        REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
        SEQUENCE_NUMBER: "1497000"
        OVERWRITE_STATE: false
        UPLOAD_EXPIRED_FILES: true
        IMPORT_NATURAL_EARTH: true
        IMPORT_OSM_LAND: true
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-05eeb57b8e67b3cd5
        AWS_ElasticBlockStore_size: 200Gi
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
    # ====================================================================================================
    # Variables for tiler-server
    # ====================================================================================================
  
    tilerServer:
      enabled: true
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      commad: './start.sh'
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: vtiles.staging.openhistoricalmap.org
      env:
        TILER_SERVER_PORT: 9090
        TILER_CACHE_TYPE: s3
        TILER_CACHE_BASEPATH: /mnt/data
        TILER_CACHE_MAX_ZOOM: 22
        # in case s3
        TILER_CACHE_REGION: us-east-1
        TILER_CACHE_BUCKET: tiler-cache-staging
        TILER_CACHE_AWS_ACCESS_KEY_ID: {{STAGING_TILER_CACHE_AWS_ACCESS_KEY_ID}}
        TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{STAGING_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
        # In case you use TILER_CACHE_TYPE: file with  persistenceDisk
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-0e8b605fe3b16bf08
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "10Gi"
          cpu: "2"
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization: 60
    # ====================================================================================================
    # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
    # ====================================================================================================
    tilerServerCacheCleaner:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      command: './cache_cleaner.sh'
      resources:
        enabled: true
        requests:
          memory: "2Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "800m"
      env:
        KILL_PROCESS: manually
        MAX_NUM_PS: 4
        PROCESS_NAME: tegola
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 1
        cpuUtilization: 90
    # ====================================================================================================
    # Variables for tiler-visor
    # ====================================================================================================
    tilerVisor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: tiler
      replicaCount: 1
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: 35.233.232.82
      env:
        TILER_VISOR_PROTOCOL: http
        TILER_VISOR_PORT: 8081
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
  
    # ====================================================================================================
    # Variables for Tasking Manager DB
    # ====================================================================================================
    tmDb:
      enabled: true
      image:
        name: "postgis/postgis"
        tag: "11-2.5"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        POSTGRES_DB: tm
        POSTGRES_PASSWORD: {{STAGING_TM_DB_PASSWORD}}
        POSTGRES_USER: postgres
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        AWS_ElasticBlockStore_volumeID: vol-05a364a565aea360c
        AWS_ElasticBlockStore_size: 50Gi
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
    # ====================================================================================================
    # Variables for Tasking Manager API
    # ====================================================================================================
  
    tmApi:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      replicaCount: 1
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: tm-api.staging.openhistoricalmap.org
      env:
        POSTGRES_HOST: {{STAGING_TM_API_DB_HOST}}
        POSTGRES_DB: {{STAGING_TM_API_DB}}
        POSTGRES_PASSWORD: {{STAGING_TM_API_DB_PASSWORD}}
        POSTGRES_USER: {{STAGING_TM_API_DB_USER}}
        POSTGRES_PORT: 5432
        TM_ORG_NAME: 'OpenHistoricalMap'
        TM_ORG_CODE: 'OHM'
        TM_ORG_URL: 'openhistoricalmap.org'
        TM_ORG_PRIVACY_POLICY_URL: 'staging.openhistoricalmap.org/copyright'
        TM_ORG_GITHUB: 'github.com/openhistoricalmap'
        OSM_SERVER_URL: 'https://staging.openhistoricalmap.org'
        OSM_NOMINATIM_SERVER_URL: 'https://nominatim-api.staging.openhistoricalmap.org'
        OSM_REGISTER_URL: 'https://staging.openhistoricalmap.org/user/new'
        ID_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=id'
        POTLATCH2_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=potlatch2'
        TM_SECRET: {{STAGING_TM_API_SECRET}}
        TM_CONSUMER_KEY: {{STAGING_TM_API_CONSUMER_KEY}}
        TM_CONSUMER_SECRET: {{STAGING_TM_API_CONSUMER_SECRET}}
        TM_EMAIL_FROM_ADDRESS: 'ohm-admins@googlegroups.com'
        TM_SMTP_HOST: 'email-smtp.us-east-1.amazonaws.com'
        TM_SMTP_PORT: 25
        TM_SMTP_USER: {{MAILER_USERNAME}}
        TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
        TM_DEFAULT_LOCALE: 'en'
        TM_APP_API_URL: 'https://tm-api.staging.openhistoricalmap.org'
        TM_APP_BASE_URL: 'https://tasks.staging.openhistoricalmap.org'
        TM_IMPORT_MAX_FILESIZE: 3000000
        TM_MAX_AOI_AREA: 15000
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
  
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
    nominatimApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: nominatim-api.staging.openhistoricalmap.org
      replicaCount: 1
      env:
        PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240612_0002.osm.pbf
        REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
        REPLICATION_UPDATE_INTERVAL: 60
        REPLICATION_RECHECK_INTERVAL: 30
        FREEZE: false
        IMPORT_WIKIPEDIA: false
        IMPORT_US_POSTCODES: false
        IMPORT_GB_POSTCODES: false
        IMPORT_TIGER_ADDRESSES: false
        THREADS: 8
        NOMINATIM_PASSWORD: {{STAGING_NOMINATIM_PG_PASSWORD}}
        PGDATA: /var/lib/postgresql/14/main
        NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
        UPDATE_MODE: continuous
        OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/14/main
        subPath: nominatim-pgdata
        # Minikube
        localVolumeHostPath: /mnt/nominatim-db-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-0a530cc12fd745545
        AWS_ElasticBlockStore_size: 100Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-disk-nominatim_db-v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
  # ====================================================================================================
  # Variables for overpass-api
  # ====================================================================================================
    overpassApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: overpass-api.staging.openhistoricalmap.org
      env:
        OVERPASS_META: 'attic'
        OVERPASS_MODE: 'init'
        OVERPASS_PLANET_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240612_0002.osm.pbf
        OVERPASS_DIFF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute
        OVERPASS_RULES_LOAD: '10'
        OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
        OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1258000'
        OVERPASS_ALLOW_DUPLICATE_QUERIES: 'yes'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        AWS_ElasticBlockStore_volumeID: vol-000676289c153e301
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: '8Gi'
          cpu: '2'
        limits:
          memory: '8Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
  # ====================================================================================================
  # Variables for taginfo
  # ====================================================================================================
    taginfo:
      enabled: true
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: taginfo.staging.openhistoricalmap.org
      env:
        URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
        URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
        URL_PLANET_FILE: 'none'
        URL_HISTORY_PLANET_FILE: 'none'
        TIME_UPDATE_INTERVAL: 7d
        OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/staging/images/taginfo/taginfo-config-staging.json
        TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
        DOWNLOAD_DB: 'languages wiki'
        CREATE_DB: 'db projects chronology'
        ENVIRONMENT: staging
        AWS_S3_BUCKET: taginfo
        INTERVAL_DOWNLOAD_DATA: 7d
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      cronjob:
        enabled: false
        schedule: "0 2 */3 * *"
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job_xlarge
        resources:
          enabled: true
          requests:
            memory: "13Gi"
            cpu: "3600m"
          limits:
            memory: "14Gi"
            cpu: "3800m"
  # ====================================================================================================
  # Variables for osm-simple-metrics
  # ====================================================================================================
    osmSimpleMetrics:
      enabled: false
      schedule: '0 2 * * *'
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
    monitoringReplication:
      enabled: false
      schedule: '*/30 * * * *'
      env:
        CREATE_MISSING_FILES: "empty"
        REPLICATION_SEQUENCE_NUMBER: "000000"
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
# ====================================================================================================
    changesetReplicationJob:
      enabled: true
      resources:
        enabled: false
        requests:
          memory: '20Gi'
          cpu: '8'
        limits:
          memory: '24Gi'
          cpu: '10'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
    osmchaWeb:
      enabled: false
# ====================================================================================================
# Variables for osmcha Api, We hard code the container id.
# ====================================================================================================
    osmchaApi:
      enabled: false
      image:
        name: "ghcr.io/openhistoricalmap/osmcha-django"
        tag: "a1bcea85dc1f7c27566c20bafe7fff7aaa1e38a4"
      ingressDomain: osmcha.staging.openhistoricalmap.org
      env:
        DJANGO_SETTINGS_MODULE: "config.settings.production"
        OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
        DJANGO_SECRET_KEY: {{STAGING_OSMCHA_DJANGO_SECRET_KEY}}        
        OAUTH_OSM_KEY: {{STAGING_OSMCHA_API_CONSUMER_KEY}}
        OAUTH_OSM_SECRET: {{STAGING_OSMCHA_API_CONSUMER_SECRET}}
        DJANGO_SECURE_SSL_REDIRECT: "False"
        OSM_SERVER_URL: https://www.openhistoricalmap.org
        OAUTH_REDIRECT_URI: https://osmcha.staging.openhistoricalmap.org/oauth-landing.html
        OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
        ## frontend
        OSMCHA_URL: https://osmcha.staging.openhistoricalmap.org
        OSMCHA_API_URL: www.openhistoricalmap.org
        REACT_APP_OSM_URL: https://www.openhistoricalmap.org
        REACT_APP_OSM_API: https://www.openhistoricalmap.org/api/0.6
        REACT_APP_OVERPASS_BASE: //overpass-api.openhistoricalmap.org/api/interpreter
        REACT_APP_ENABLE_REAL_CHANGESETS: 0
        REACT_APP_MAPBOX_ACCESS_TOKEN: {{STAGING_OSMCHA_REACT_APP_MAPBOX_ACCESS_TOKEN}}
      resources:
        enabled: false
        requests:
          memory: "512Mi"
          cpu: "1"
        limits:
          memory: "512Mi"
          cpu: "1"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
    osmchaDb:
      enabled: false
      image:
        name: "developmentseed/osmseed-osmcha-db"
        tag: "0.1.0-n767.h0090e97"
      env:
        POSTGRES_DB: {{STAGING_OSMCHA_PG_DATABASE}}
        POSTGRES_USER: {{STAGING_OSMCHA_PG_USER}}
        POSTGRES_PASSWORD: {{STAGING_OSMCHA_PG_PASSWORD}}
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        # Minikube
        localVolumeHostPath: /mnt/db-data/osmcha-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-06f981afe8def5915
        AWS_ElasticBlockStore_size: 10Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-osmcha-disk--v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: false
# ====================================================================================================
# Planet files server
# ====================================================================================================
    planetFiles:
      enabled: false
      