osm-seed:

    # ====================================================================================================
    # ====================================================================================================
    # ==================================Global Configurations=============================================
    # ====================================================================================================
    # ====================================================================================================
    
    # The version of the image group in osm-seed, get it here: https://hub.docker.com/r/developmentseed/osmseed-web/tags/
    # osmSeedVersion: ohm-b8a0ed1
  
    environment: staging
    # cloudProvider is provider where you are going to deploy osm-seed, it could be: aws, gcp, minikube
    cloudProvider: aws
  
    # ====================================================================================================
    # AWS: In case you are using the cloudProvider=aws set the below variables, We are assuming the nodes has a policies access to S3
    # ====================================================================================================
    AWS_S3_BUCKET: {{STAGING_S3_BUCKET}}
  
    # ====================================================
    # AWS: Specify ARN for SSL certificate, currently assumes a single wildcard cert
    # ====================================================
  
    AWS_SSL_ARN: {{AWS_SSL_ARN}}
  
    # Specify serviceType.
    #
    # serviceType can be one of three values: 'NodePort', 'ClusterIP' or 'LoadBalancer'
    # Use `NodePort` for local testing on minikube.
    #
    # The recommended setting is `ClusterIP`, and then following the instructions to
    # point a DNS record to the cluster IP address. This will setup the ingress rules
    # for all services as subdomains and configure SSL using Lets Encrypt.
    #
    # If you specify `LoadBalancer` as the service type, if you also specify
    # an `AWS_SSL_ARN` that is a wildcart certificate, that will be configured
    # as the SSL certificate for your services. Else, you will need to configure
    # SSL separately. 
    serviceType: ClusterIP
    createClusterIssuer: true
    # Domain that is pointed to the clusterIP
    # You will need to create an A record like *.osmseed.example.com pointed to the ClusterIP
    # Then, the cluster configuration will setup services at their respective subdomains:
    # - web.osmseed.example.com
    # - overpass.osmseed.example.com
    # - nominatim.osmseed.example.com
    # - etc.
    domain: staging.openhistoricalmap.org
  
    # ====================================================================================================
    # Configuration for Lets Encrypt setup
    # ====================================================================================================
  
    # Admin Email address used when generating Lets Encrypt certificates.
    # You will be notified of expirations, etc. on this email address.
    adminEmail: ohm-admins@googlegroups.com
    # ====================================================================================================
    # ====================================================================================================
    # ==================================Pods Configurations===============================================
    # ====================================================================================================
    # ====================================================================================================
    # ====================================================================================================
    # Variables for osm-seed database
    # ====================================================================================================
    db:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
      env:
        POSTGRES_DB: {{STAGING_DB}}
        POSTGRES_USER: {{STAGING_DB_USER}}
        POSTGRES_PASSWORD: {{STAGING_DB_PASSWORD}}
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-db
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : vol-0d644eb9ea4e8fccf
        AWS_ElasticBlockStore_size: 200Gi
      resources:
        enabled: false
        requests:
          memory: "6Gi"
          cpu: "1500m"
        limits:
          memory: "6Gi"
          cpu: "1700m"
      sharedMemorySize: 256Mi
      livenessProbeExec: true
      postgresqlConfig:
        enabled: true
        values: |
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 2GB
          work_mem = 20MB
          maintenance_work_mem = 512MB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 1GB
          min_wal_size = 80MB
          random_page_cost = 1.1
          effective_cache_size = 6GB
          log_min_duration_statement = 3000
          log_connections = on
          log_disconnections = on
          log_duration = off
          log_lock_waits = on
          log_statement = 'none'
          log_timezone = 'Etc/UTC'
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'
          # Parallelism settings
          max_parallel_workers_per_gather = 2
          max_parallel_workers = 4
          max_worker_processes = 4
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB
          session_preload_libraries = 'auto_explain'
          auto_explain.log_min_duration = '3s'
    # ====================================================================================================
    # Variables for osm-seed website
    # ====================================================================================================
    web:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      replicaCount: 1
      # Set staticIp, if you are using cloudProvider=gcp
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: www.staging.openhistoricalmap.org
      env:
        MAILER_ADDRESS: {{MAILER_ADDRESS}}
        MAILER_DOMAIN: staging.openhistoricalmap.org
        MAILER_USERNAME: {{MAILER_USERNAME}}
        MAILER_PASSWORD: {{MAILER_PASSWORD}}
        OSM_id_key: {{STAGING_ID_APPLICATION}}
        OAUTH_CLIENT_ID: {{STAGING_OAUTH_CLIENT_ID}}
        OAUTH_KEY: {{STAGING_OAUTH_KEY}}
        MAILER_FROM: ohm-admins@googlegroups.com
        NOMINATIM_URL: nominatim-api.staging.openhistoricalmap.org
        OVERPASS_URL: overpass-api.staging.openhistoricalmap.org	
        NEW_RELIC_LICENSE_KEY: "none"
        NEW_RELIC_APP_NAME: "none"
        ORGANIZATION_NAME: OpenHistoricalMap
        WEBSITE_STATUS: "online"
        RAILS_CREDENTIALS_YML_ENC: {{STAGING_RAILS_CREDENTIALS_YML_ENC}}
        RAILS_MASTER_KEY: {{STAGING_RAILS_MASTER_KEY}}
        # API_TIMEOUT: 600
        # WEB_TIMEOUT: 600
      resources:
        enabled: true
        requests:
          memory: "2Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "600m"
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 10
        cpuUtilization: 80
      sharedMemorySize: 16Mi
    # ====================================================================================================
    # Variables for memcached. Memcached is used to store session cookies
    # ====================================================================================================
    memcached:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      resources:
        enabled: false
        requests:
          memory: "8Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "2"
    # ====================================================================================================
    # Variables for osm-seed for osmosis, this configuration os to get the planet dump files from apidb
    # ====================================================================================================
    planetDump:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      schedule: '0 0 * * *'
      env:
        OVERWRITE_PLANET_FILE: true
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for full-history container
    # ====================================================================================================
    fullHistory:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      schedule: '0 0 * * *'
      env:
        OVERWRITE_FHISTORY_FILE: false
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"

    # ====================================================================================================
    # Variables for replication-job, Configuration to create the replication files by, minute, hour, or day
    # ====================================================================================================
    replicationJob:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
      env:
        ENABLE_SEND_SLACK_MESSAGE: "true"
        SLACK_WEBHOOK_URL: {{OHM_SLACK_WEBHOOK_URL}}
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
  
    # ====================================================================================================
    # Variables for osm-seed to pupulate the apidb
    # ====================================================================================================
    populateApidb:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env:
        URL_FILE_TO_IMPORT: 'https://storage.googleapis.com/osm-seed/osm-processor/history-latest-to-import-output.pbf'
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2.5"
  
    # ====================================================================================================
    # Variables to start a pod to process osm files
    # ====================================================================================================
    osmProcessor:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
      env: 
        URL_FILE_TO_PROCESS: 'https://storage.googleapis.com/osm-seed/planet/full-history/history-latest-to-import.pbf'
        OSM_FILE_ACTION: simple_pbf
      resources:
        enabled: false
        requests:
          memory: "14Gi"
          cpu: "4"
        limits:
          memory: "16Gi"
          cpu: "4"
  
    # ====================================================================================================
    # Variables for restoring the DB
    # ====================================================================================================
    dbBackupRestore:
      cronjobs:
      - name: web-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/web-api-db
          BACKUP_CLOUD_FILE: ohm-api-web-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
          RESTORE_URL_FILE: https://osmseed.s3.amazonaws.com/test.sql.gz
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: db_large
      - name: tm-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: true
          BACKUP_CLOUD_FOLDER: database/tm-db
          BACKUP_CLOUD_FILE: ohm-tm-db
          AWS_S3_BUCKET: osmseed-staging
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: db_large
      - name: osmcha-db
        enabled: false
        schedule: '0 1 * * *'
        env:
          # backup/restore
          DB_ACTION: backup
          # Naming backup files
          SET_DATE_AT_NAME: 'true'
          BACKUP_CLOUD_FOLDER: database/osmcha-db
          BACKUP_CLOUD_FILE: osmseed-osmcha-db
          AWS_S3_BUCKET: osmseed-staging
          # Clean up backups options
          CLEANUP_BACKUPS: true
          RETENTION_DAYS: '30'
        resources:
          enabled: false
          requests:
            memory: '300Mi'
            cpu: '0.5'
          limits:
            memory: '400Mi'
            cpu: '0.6'
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job

    # ====================================================================================================
    # Variables for tiler-db
    # ====================================================================================================
  
    tilerDb:
      enabled: true
      useExternalHost: # When we are using useExternalHost.enabled= true other variables are giong to be disable ans use the external host config
        enabled: true
      env:
        POSTGRES_HOST: {{STAGING_TILER_DB_HOST}}
        POSTGRES_DB: tiler_osm_production # Kuberntes existing db called,  tiler-osm
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_TILER_DB_PASSWORD}}
        POSTGRES_PORT: 5432
      sharedMemorySize: 128Mi
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID : vol-070a59c4b4d7c0b32
        AWS_ElasticBlockStore_size: 200Gi
      resources:
          enabled: false
          requests:
            memory: 6Gi
            cpu: 1500m
          limits:
            memory: 6Gi
            cpu: 1700m
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
      postgresqlConfig:
        enabled: true
        values: |
          # PostgreSQL configuration
          listen_addresses = '*'
          max_connections = 100
          shared_buffers = 4GB
          work_mem = 128MB
          maintenance_work_mem = 1GB
          dynamic_shared_memory_type = posix
          effective_io_concurrency = 200
          max_wal_size = 2GB
          min_wal_size = 256MB
          random_page_cost = 1.1
          effective_cache_size = 12GB

          # Logging Settings
          log_min_duration_statement = 15000  # Log queries that take more than 15 seconds
          log_connections = on               # Log database connections
          log_disconnections = on            # Log database disconnections
          log_duration = off                 # Do not log the duration of all queries
          log_lock_waits = on                # Log when queries are waiting for locks
          log_statement = 'none'             # Avoid logging all queries by default
          log_timezone = 'Etc/UTC'

          # Locale Settings
          datestyle = 'iso, mdy'
          timezone = 'Etc/UTC'
          lc_messages = 'en_US.utf8'
          lc_monetary = 'en_US.utf8'
          lc_numeric = 'en_US.utf8'
          lc_time = 'en_US.utf8'
          default_text_search_config = 'pg_catalog.english'

          # Parallelism Settings
          max_parallel_workers_per_gather = 4
          max_parallel_workers = 8
          max_worker_processes = 8
          parallel_tuple_cost = 0.1
          parallel_setup_cost = 1000
          min_parallel_table_scan_size = 8MB
          min_parallel_index_scan_size = 512kB

          # Shared Libraries for Monitoring and Optimization
          shared_preload_libraries = 'auto_explain,pg_stat_statements'
          auto_explain.log_min_duration = '15s'  # Log queries explained by auto_explain that take more than 15 seconds

          # Timeout Settings
          tcp_keepalives_idle = 300
          tcp_keepalives_interval = 60
          tcp_keepalives_count = 10
          enable_mergejoin = false  # Disable merge joins for specific query patterns
          enable_hashjoin = false   # Disable hash joins for specific query patterns
          statement_timeout = '600s'  # Maximum time allowed for a query to complete (10 minutes)
          lock_timeout = '60s'        # Maximum time to wait for a lock (60 seconds)
          idle_in_transaction_session_timeout = '600s'  # Terminate idle transactions after 10 minutes

          # pg_stat_statements Settings
          pg_stat_statements.max = 10000  # Maximum number of queries tracked in pg_stat_statements
          pg_stat_statements.track = all  # Track all queries (DDL, DML, etc.)


    # ====================================================================================================
    # Variables for tiler-imposm
    # ====================================================================================================
  
    tilerImposm:
      enabled: false
      env:
        TILER_IMPORT_FROM: osm
        TILER_IMPORT_PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-250103_0001.osm.pbf
        REPLICATION_URL: http://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute/
        SEQUENCE_NUMBER: '1683500'
        OVERWRITE_STATE: false
        UPLOAD_EXPIRED_FILES: true
        IMPORT_NATURAL_EARTH: true
        IMPORT_OSM_LAND: true
        IMPOSM3_IMPORT_LAYERS: all
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-0b7624ded1ed38af9
        AWS_ElasticBlockStore_size: 20Gi
      resources:
        enabled: false
        requests:
          memory: 20Gi
          cpu: '8'
        limits:
          memory: 24Gi
          cpu: '10'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
    # ====================================================================================================
    # Variables for tiler-server
    # ====================================================================================================
  
    tilerServer:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
      replicaCount: 1
      commad: './start.sh'
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: vtiles.staging.openhistoricalmap.org
      env:
        TILER_SERVER_PORT: 9090
        TILER_CACHE_TYPE: s3
        TILER_CACHE_BASEPATH: /mnt/data
        TILER_CACHE_MAX_ZOOM: 22
        # in case s3
        TILER_CACHE_REGION: us-east-1
        TILER_CACHE_BUCKET: tiler-cache-staging
        TILER_CACHE_AWS_ACCESS_KEY_ID: {{STAGING_TILER_CACHE_AWS_ACCESS_KEY_ID}}
        TILER_CACHE_AWS_SECRET_ACCESS_KEY: {{STAGING_TILER_CACHE_AWS_SECRET_ACCESS_KEY}}
        # In case you use TILER_CACHE_TYPE: file with  persistenceDisk
        EXECUTE_REINDEX: false
        EXECUTE_VACUUM_ANALYZE: false
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /mnt/data
        # In case cloudProvider: aws
        AWS_ElasticBlockStore_volumeID: vol-0e8b605fe3b16bf08
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: 2Gi
          cpu: '1'
        limits:
          memory: 10Gi
          cpu: '2'
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 2
        cpuUtilization: 60
    # ====================================================================================================
    # Variables for tiler-server cache cleaner, only avaliable in case the TILER_CACHE_TYPE = s3  
    # ====================================================================================================
    tilerServerCacheCleaner:
      enabled: false
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      replicaCount: 1
      command: './cache_cleaner.sh' 
      resources:
        enabled: true
        requests:
          memory: 1Gi
          cpu: '500m'
        limits:
          memory: 1Gi
          cpu: '600m'
      env:
        KILL_PROCESS: manually
        MAX_NUM_PS: 4
        PROCESS_NAME: tegola
      autoscaling:
        enabled: false
        minReplicas: 1
        maxReplicas: 1
        cpuUtilization: 90
  
    # ====================================================================================================
    # Variables for Tasking Manager DB
    # ====================================================================================================
    tmDb:
      enabled: true
      image:
        name: "postgis/postgis"
        tag: "11-2.5"
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: db_large
      env:
        POSTGRES_DB: tm
        POSTGRES_PASSWORD: {{STAGING_TM_DB_PASSWORD}}
        POSTGRES_USER: postgres
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        subPath: postgresql-d
        AWS_ElasticBlockStore_volumeID: vol-077f1a9f62a1951e8
        AWS_ElasticBlockStore_size: 50Gi
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
    # ====================================================================================================
    # Variables for Tasking Manager API
    # ====================================================================================================
  
    tmApi:
      enabled: true
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      replicaCount: 1
      staticIp: c
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: tm-api.staging.openhistoricalmap.org
      env:
        TM_ORG_NAME: 'OpenHistoricalMap'
        TM_ORG_CODE: 'OHM'
        TM_ORG_URL: 'openhistoricalmap.org'
        TM_ORG_PRIVACY_POLICY_URL: 'staging.openhistoricalmap.org/copyright'
        TM_ORG_GITHUB: 'github.com/openhistoricalmap'
        OSM_SERVER_URL: 'https://staging.openhistoricalmap.org'
        OSM_NOMINATIM_SERVER_URL: 'https://nominatim-api.staging.openhistoricalmap.org'
        OSM_REGISTER_URL: 'https://staging.openhistoricalmap.org/user/new'
        ID_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=id'
        POTLATCH2_EDITOR_URL: 'https://staging.openhistoricalmap.org/edit?editor=potlatch2'
        TM_SECRET: {{STAGING_TM_API_SECRET}}
        TM_CONSUMER_KEY: {{STAGING_TM_API_CONSUMER_KEY}}
        TM_CONSUMER_SECRET: {{STAGING_TM_API_CONSUMER_SECRET}}
        TM_EMAIL_FROM_ADDRESS: 'ohm-admins@googlegroups.com'
        TM_SMTP_HOST: 'email-smtp.us-east-1.amazonaws.com'
        TM_SMTP_PORT: 25
        TM_SMTP_USER: {{MAILER_USERNAME}}
        TM_SMTP_PASSWORD: {{MAILER_PASSWORD}}
        TM_DEFAULT_LOCALE: 'en'
        TM_APP_API_URL: 'https://tm-api.staging.openhistoricalmap.org'
        TM_APP_BASE_URL: 'https://tasks-staging.openhistoricalmap.org'
        TM_IMPORT_MAX_FILESIZE: 3000000
        TM_MAX_AOI_AREA: 15000
        TM_APP_API_VERSION: v2
        # The following environment variables are for future versions of TM
        TM_CLIENT_ID: {{STAGING_TM_CLIENT_ID}}
        TM_CLIENT_SECRET: {{STAGING_TM_CLIENT_SECRET}}
        TM_DEFAULT_CHANGESET_COMMENT: STAGING
        TM_REDIRECT_URI: https://tm-api.staging.openhistoricalmap.org/authorized
        TM_SCOPE: 'read_prefs write_api'
        # Add extra info
        TM_EMAIL_CONTACT_ADDRESS: ohm-admins@googlegroups.com
        TM_ORG_FB: https://www.facebook.com/openhistoricalmap
        TM_ORG_INSTAGRAM: https://www.openhistoricalmap.org
        TM_ORG_TWITTER: https://x.com/OpenHistMap
        TM_ORG_YOUTUBE: https://www.youtube.com/playlist?list=PLOi35w6_Hpx_CYdYBUpPeuiJ1djn5-wIx
      resources:
        enabled: false
        requests:
          memory: "1Gi"
          cpu: "2"
        limits:
          memory: "2Gi"
          cpu: "2"
  
  # ====================================================================================================
  # Variables for nominatim api
  # ====================================================================================================
    nominatimApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: nominatim-api.staging.openhistoricalmap.org
      replicaCount: 1
      env:
        PBF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240612_0002.osm.pbf
        REPLICATION_URL: http://planet.openhistoricalmap.org.s3.amazonaws.com/replication/minute
        REPLICATION_UPDATE_INTERVAL: 60
        REPLICATION_RECHECK_INTERVAL: 30
        FREEZE: false
        IMPORT_WIKIPEDIA: false
        IMPORT_US_POSTCODES: false
        IMPORT_GB_POSTCODES: false
        IMPORT_TIGER_ADDRESSES: false
        THREADS: 8
        NOMINATIM_PASSWORD: {{STAGING_NOMINATIM_PG_PASSWORD}}
        PGDATA: /var/lib/postgresql/14/main
        NOMINATIM_ADDRESS_LEVEL_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/nominatim-ui/master/address-levels.json
        UPDATE_MODE: continuous
        OSMSEED_WEB_API_DOMAIN: www.openhistoricalmap.org
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/14/main
        subPath: nominatim-pgdata
        # Minikube
        localVolumeHostPath: /mnt/nominatim-db-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-0a530cc12fd745545
        AWS_ElasticBlockStore_size: 100Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-disk-nominatim_db-v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
  # ====================================================================================================
  # Variables for overpass-api
  # ====================================================================================================
    overpassApi:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: overpass-api.staging.openhistoricalmap.org
      env:
        OVERPASS_META: 'attic'
        OVERPASS_MODE: 'init'
        OVERPASS_PLANET_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/planet-240612_0002.osm.pbf
        OVERPASS_DIFF_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/minute
        OVERPASS_RULES_LOAD: '10'
        OVERPASS_PLANET_PREPROCESS: 'mv /db/planet.osm.bz2 /db/planet.osm.pbf && osmium cat -o /db/planet.osm.bz2 /db/planet.osm.pbf && rm /db/planet.osm.pbf'
        OVERPASS_REPLICATION_SEQUENCE_NUMBER: '1258000'
        OVERPASS_ALLOW_DUPLICATE_QUERIES: 'yes'
      persistenceDisk:
        enabled: true
        accessMode: ReadWriteOnce
        AWS_ElasticBlockStore_volumeID: vol-000676289c153e301
        AWS_ElasticBlockStore_size: 100Gi
      resources:
        enabled: false
        requests:
          memory: '8Gi'
          cpu: '2'
        limits:
          memory: '8Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job_xlarge
  # ====================================================================================================
  # Variables for taginfo
  # ====================================================================================================
    taginfo:
      enabled: false
      serviceAnnotations:
        service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "300"
      ingressDomain: taginfo.staging.openhistoricalmap.org
      env:
        URL_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/state.txt
        URL_HISTORY_PLANET_FILE_STATE: https://s3.amazonaws.com/planet.openhistoricalmap.org/planet/full-history/state.txt
        URL_PLANET_FILE: 'none'
        URL_HISTORY_PLANET_FILE: 'none'
        TIME_UPDATE_INTERVAL: 7d
        OVERWRITE_CONFIG_URL: https://raw.githubusercontent.com/OpenHistoricalMap/ohm-deploy/staging/images/taginfo/taginfo-config-staging.json
        TAGINFO_PROJECT_REPO: https://github.com/OpenHistoricalMap/taginfo-projects.git
        DOWNLOAD_DB: 'languages wiki'
        CREATE_DB: 'db projects chronology'
        ENVIRONMENT: staging
        AWS_S3_BUCKET: taginfo
        INTERVAL_DOWNLOAD_DATA: 7d
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web_large
      cronjob:
        enabled: false
        schedule: "0 2 */3 * *"
        nodeSelector:
          enabled: true
          label_key: nodegroup_type
          label_value: job_xlarge
        resources:
          enabled: true
          requests:
            memory: "13Gi"
            cpu: "3600m"
          limits:
            memory: "14Gi"
            cpu: "3800m"
  # ====================================================================================================
  # Variables for osm-simple-metrics
  # ====================================================================================================
    osmSimpleMetrics:
      enabled: false
      schedule: '0 2 * * *'
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: job

  # ====================================================================================================
  # Variables for replication nomitoring task
  # ====================================================================================================
    monitoringReplication:
      enabled: false
      schedule: '*/30 * * * *'
      env:
        CREATE_MISSING_FILES: "empty"
        REPLICATION_SEQUENCE_NUMBER: "000000"
      resources:
        enabled: false
        requests:
          memory: '1Gi'
          cpu: '2'
        limits:
          memory: '2Gi'
          cpu: '2'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for changeset-replication-job, Configuration to create the replication files by, minute, hour, or day
# ====================================================================================================
    changesetReplicationJob:
      enabled: false
      resources:
        enabled: false
        requests:
          memory: '20Gi'
          cpu: '8'
        limits:
          memory: '24Gi'
          cpu: '10'
      nodeSelector:
        enabled: true
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha web
# ====================================================================================================
    osmchaWeb:
      enabled: false
# ====================================================================================================
# Variables for osmcha Api, We hard code the container id.
# ====================================================================================================
    osmchaApi:
      enabled: false
      image:
        name: "ghcr.io/openhistoricalmap/osmcha-django"
        tag: "a1bcea85dc1f7c27566c20bafe7fff7aaa1e38a4"
      ingressDomain: osmcha.staging.openhistoricalmap.org
      env:
        DJANGO_SETTINGS_MODULE: "config.settings.production"
        OSMCHA_FRONTEND_VERSION: "v0.86.0-production"
        DJANGO_SECRET_KEY: {{STAGING_OSMCHA_DJANGO_SECRET_KEY}}        
        OAUTH_OSM_KEY: {{STAGING_OSMCHA_API_CONSUMER_KEY}}
        OAUTH_OSM_SECRET: {{STAGING_OSMCHA_API_CONSUMER_SECRET}}
        DJANGO_SECURE_SSL_REDIRECT: "False"
        OSM_SERVER_URL: https://www.openhistoricalmap.org
        OAUTH_REDIRECT_URI: https://osmcha.staging.openhistoricalmap.org/oauth-landing.html
        OSM_PLANET_BASE_URL: https://s3.amazonaws.com/planet.openhistoricalmap.org/replication/changesets/
        ## frontend
        OSMCHA_URL: https://osmcha.staging.openhistoricalmap.org
        OSMCHA_API_URL: www.openhistoricalmap.org
        REACT_APP_OSM_URL: https://www.openhistoricalmap.org
        REACT_APP_OSM_API: https://www.openhistoricalmap.org/api/0.6
        REACT_APP_OVERPASS_BASE: //overpass-api.openhistoricalmap.org/api/interpreter
        REACT_APP_ENABLE_REAL_CHANGESETS: 0
        REACT_APP_MAPBOX_ACCESS_TOKEN: {{STAGING_OSMCHA_REACT_APP_MAPBOX_ACCESS_TOKEN}}
      resources:
        enabled: false
        requests:
          memory: "512Mi"
          cpu: "1"
        limits:
          memory: "512Mi"
          cpu: "1"
      nodeSelector:
        enabled: false
        label_key: nodegroup_type
        label_value: web
# ====================================================================================================
# Variables for osmcha DB
# ====================================================================================================
    osmchaDb:
      enabled: false
      image:
        name: "developmentseed/osmseed-osmcha-db"
        tag: "0.1.0-n767.h0090e97"
      env:
        POSTGRES_DB: osmcha
        POSTGRES_USER: postgres
        POSTGRES_PASSWORD: {{STAGING_OSMCHA_PG_PASSWORD}}
      resources:
        enabled: false
        requests:
          memory: "20Gi"
          cpu: "8"
        limits:
          memory: "24Gi"
          cpu: "10"
      persistenceDisk:
        enabled: false
        accessMode: ReadWriteOnce
        mountPath: /var/lib/postgresql/data
        # Minikube
        localVolumeHostPath: /mnt/db-data/osmcha-data
        localVolumeSize: 10Gi
        # AWS
        AWS_ElasticBlockStore_volumeID: vol-06f981afe8def5915
        AWS_ElasticBlockStore_size: 10Gi
        # GCP
        GCP_gcePersistentDisk_pdName: osmseed-osmcha-disk--v1
        GCP_gcePersistentDisk_size: 50Gi
      nodeSelector:
        enabled: false
# ====================================================================================================
# Planet files server
# ====================================================================================================
    planetFiles:
      enabled: false

# ====================================================================================================
# Tiles cache SQS processor
# ====================================================================================================
ohm:
  tilerCache:
    enabled: true
    
  tilerCachePurge:
    enabled: true
    env:
      REGION_NAME: us-east-1
      NAMESPACE: default # Namespace to run the job
      DOCKER_IMAGE: ghcr.io/openhistoricalmap/tiler-server:0.0.1-0.dev.git.1967.h8492956 # TODO, this should be automatically updated from tiler server image
      SQS_QUEUE_URL: {{STAGING_SQS_QUEUE_URL}}
      NODEGROUP_TYPE: web_large # Nodegroup type to run the purge and seed job
      # Maximum number of active jobs in high concurrency queue
      MAX_ACTIVE_JOBS: 10 
      DELETE_OLD_JOBS_AGE: 3600 # 1 hours
      ## Execute purging
      EXECUTE_PURGE: true
      PURGE_CONCURRENCY: 64
      PURGE_MIN_ZOOM: 3
      PURGE_MAX_ZOOM: 12 # Purging zoom 15,16,17,18,19,20 takes hours to complete,we are going to remove direct from s3 the tiles for zoom 19-20
      ## Execute seeding
      EXECUTE_SEED: false
      SEED_CONCURRENCY: 64
      SEED_MIN_ZOOM: 0
      SEED_MAX_ZOOM: 12
      ## Remove tiles from s3 for zoom levels
      ZOOM_LEVELS_TO_DELETE: 13,14,15,16,17,18,19,20
      S3_BUCKET_CACHE_TILER: tiler-cache-staging
      S3_BUCKET_PATH_FILES: mnt/data/osm
    resources:
      enabled: false
      requests:
        memory: 20Gi
        cpu: '8'
      limits:
        memory: 24Gi
        cpu: '10'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_large

  # Tiler seed by default is giong to seet tiles from 0-5 zoom level
  tilerCacheSeed:
    enabled: true
    schedule: '* * * * *'
    env:
      GEOJSON_URL: https://osmseed-dev.s3.us-east-1.amazonaws.com/tiler/wold-usa-eu.geojson
      ZOOM_LEVELS: 8,9,10
      CONCURRENCY: 128
      S3_BUCKET: osmseed-dev
      OUTPUT_FILE: /logs/tiler_benchmark.log
    resources:
      enabled: false
      requests:
        memory: 20Gi
        cpu: '8'
      limits:
        memory: 24Gi
        cpu: '10'
    nodeSelector:
      enabled: true
      label_key: nodegroup_type
      label_value: web_large
